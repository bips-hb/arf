% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/adversarial_rf.R
\name{adversarial_rf}
\alias{adversarial_rf}
\title{Adversarial random forest}
\usage{
adversarial_rf(
  x,
  num_trees = 10,
  min_node_size = 5,
  delta = 0,
  max_iters = 10,
  verbose = TRUE,
  parallel = TRUE,
  ...
)
}
\arguments{
\item{x}{Input data.}

\item{num_trees}{Number of trees to grow in each forest. The default works
well for most generative modeling tasks, but should be increased for
likelihood estimation. See Details.}

\item{min_node_size}{Minimum size for terminal nodes.}

\item{delta}{Tolerance parameter. Algorithm converges when OOB accuracy is
< 0.5 + \code{delta}.}

\item{max_iters}{Maximum iterations for the adversarial loop.}

\item{verbose}{Print discriminator accuracy after each round?}

\item{parallel}{Train in parallel? Must register backend beforehand, e.g.
via \code{doParallel}.}

\item{...}{Extra parameters to be passed to \code{ranger}.}
}
\value{
A random forest object of class \code{ranger}.
}
\description{
Implements an adversarial random forest to learn independence-inducing splits.
}
\details{
The adversarial random forest (ARF) algorithm partitions data into fully
factorized leaves where features are jointly independent. Call this the local
independence criterion. The ARF procedure is iterative, with alternating
rounds of generation and discrimination. In the first instance, synthetic
data is generated via independent bootstraps of each feature. A RF classifier
is trained to distinguish between real and synthetic samples. In subsequent
rounds, synthetic data is generated separately in each leaf, using splits
from the previous forest. This creates increasingly realistic data that
satisfies the local independence criterion by construction. The algorithm
converges when a RF cannot reliably distinguish between the two classes,
i.e. when OOB accuracy falls below 0.5 + \code{delta}.

ARFs are useful for several unsupservised learning tasks, such as density
estimation (see \code{\link{forde}}) and data synthesis (see
\code{\link{forge}}). For the former, we recommend increasing the number of
trees for improved performance (typically on the order of 100-1000 depending
on sample size).

Note: convergence is not guaranteed in finite samples. The \code{max_iter}
argument sets an upper bound on the number of training rounds. Similar
results may be attained by increasing \code{delta}. Even a single round can
often give good performance, but data with strong or complex dependencies may
require more iterations.
}
\examples{
arf <- adversarial_rf(iris)


}
\references{
Watson, D., Blesch, K., Kapar, J., & Wright, M. (2022). Adversarial random
forests for density estimation and generative modeling. \emph{arXiv} preprint,
2205.09435.
}
\seealso{
\code{\link{forde}}, \code{\link{forge}}
}
