[{"path":"https://bips-hb.github.io/arf/articles/arf.html","id":"adversarial-training","dir":"Articles","previous_headings":"","what":"Adversarial Training","title":"Adversarial Random Forests","text":"ARF algorithm iterative procedure. first instance, generate synthetic data independently sampling marginals feature training random forest (RF) distinguish original synthetic samples. accuracy greater 0.5+Œ¥0.5 + \\delta (delta user-controlled tolerance parameter, generally set 0), create new dataset sampling marginals within leaf training another RF classifier. procedure repeats original synthetic samples reliably distinguished. default verbose = TRUE, algorithm print accuracy iteration. printouts can turned setting verbose = FALSE. Accuracy still stored within arf object, can evaluate convergence fact. warning appears just per session. can suppressed setting parallel = FALSE registering parallel backend ().  find quick drop accuracy following resampling procedure, desired. ARF converged, resulting splits form fully factorized leaves, .e.¬†subregions feature space variables locally independent. ARF convergence asymptotically guaranteed n‚Üí‚àûn \\rightarrow \\infty (see Watson et al., 2023, Thm. 1). However, implications finite sample performance. practice, often find adversarial training completes just one two rounds, may hold datasets. avoid infinite loops, users can increase slack parameter delta set max_iters argument (default = 10). addition failsafes, adversarial_rf uses early stopping default (early_stop = TRUE), terminates training factorization improve one round next. recommended, since discriminator accuracy rarely falls much lower increased. density estimation tasks, recommend increasing default number trees. generally use 100 experiments, though may suboptimal datasets. Likelihood estimates sensitive parameter certain threshold, larger models incur extra costs time memory. can speed computations registering parallel backend, case ARF training distributed across cores using ranger package. Much like ranger, default behavior adversarial_rf compute parallel possible. exactly done varies across operating systems. following code works Unix machines. Windows requires different setup. either case, can now execute parallel. result object class ranger, can input downstream functions.","code":"# Load libraries library(arf) library(data.table) library(ggplot2)  # Set seed set.seed(123, \"L'Ecuyer-CMRG\")  # Train ARF arf_iris <- adversarial_rf(iris) #> Iteration: 0, Accuracy: 76.09% #> Iteration: 1, Accuracy: 40.33% #> Warning: executing %dopar% sequentially: no parallel backend registered # Train ARF with no printouts arf_iris <- adversarial_rf(iris, verbose = FALSE)  # Plot accuracy against iterations (model converges when accuracy <= 0.5) tmp <- data.frame('Accuracy' = arf_iris$acc,                    'Iteration' = seq_len(length(arf_iris$acc))) ggplot(tmp, aes(Iteration, Accuracy)) +    geom_point() +    geom_path() +   geom_hline(yintercept = 0.5, linetype = 'dashed', color = 'red') # Register cores - Unix library(doParallel) registerDoParallel(cores = 2) # Register cores - Windows library(doParallel) cl <- makeCluster(2) registerDoParallel(cl) # Rerun ARF, now in parallel and with more trees arf_iris <- adversarial_rf(iris, num_trees = 100) #> Iteration: 0, Accuracy: 85.67% #> Iteration: 1, Accuracy: 40.67%"},{"path":"https://bips-hb.github.io/arf/articles/arf.html","id":"parameter-learning","dir":"Articles","previous_headings":"","what":"Parameter Learning","title":"Adversarial Random Forests","text":"next step learn leaf distribution parameters using forests density estimation (FORDE). function calculates coverage, bounds, pdf/pmf parameters every variable every leaf. can expensive computation large datasets, requires ùí™(B‚ãÖd‚ãÖn‚ãÖlog(n))\\mathcal{O}\\big(B \\cdot d \\cdot n \\cdot \\log(n)\\big) operations, BB number trees, dd data dimensionality, nn sample size. , process parallelized default. Default behavior use truncated normal distribution continuous data (boundaries given tree‚Äôs split parameters) multinomial distribution categorical data. find produces stable results wide range settings. can also use uniform distribution continuous features setting family = 'unif', thereby instantiating piecewise constant density estimator. method tends perform poorly practice, recommend . option implemented primarily benchmarking purposes. Alternative families, e.g.¬†truncated Poisson beta distributions, may useful certain problems. Future releases expand range options family argument. alpha epsilon arguments allow optional regularization multinomial uniform distributions, respectively. help prevent zero likelihood samples test data fall outside support training data. former pseudocount parameter applies Laplace smoothing within leaves, preventing unobserved values assigned zero probability unless splits explicitly rule . words, impose flat Dirichlet prior report posterior probabilities rather maximum likelihood estimates. latter slack parameter empirical bounds expands estimated extrema continuous features factor 1+œµ1 + \\epsilon. Compare results original probability estimates Species variable obtained adding pseudocount Œ±=0.1\\alpha = 0.1. Laplace smoothing, extreme probabilities occur splits explicitly demand . Otherwise, values shrink toward uniform prior. Note two data tables may exactly rows, omit zero probability events conserve memory. However, can verify probabilities sum unity leaf-variable combination. forde function outputs list length 6, entries (1) continuous features; (2) categorical features; (3) leaf parameters; (4) variable metadata; (5) factor levels; (6) data input class. parameters can used variety downstream tasks, likelihood estimation data synthesis.","code":"# Compute leaf and distribution parameters params_iris <- forde(arf_iris, iris) # Recompute with uniform density params_unif <- forde(arf_iris, iris, family = 'unif') #> Warning in forde(arf_iris, iris, family = \"unif\"): Density estimation with #> uniform distribution requires finite bounds. Resetting finite_bounds to #> \"local\". # Recompute with additive smoothing params_alpha <- forde(arf_iris, iris, alpha = 0.1)  # Compare results head(params_iris$cat) #> Key: <f_idx, variable> #>    f_idx variable       val  prob NA_share #>    <int>   <char>    <char> <num>    <num> #> 1:     1  Species virginica     1        0 #> 2:     2  Species virginica     1        0 #> 3:     3  Species virginica     1        0 #> 4:     4  Species virginica     1        0 #> 5:     5  Species    setosa     1        0 #> 6:     6  Species    setosa     1        0 head(params_alpha$cat) #> Key: <f_idx, variable> #>    f_idx variable        val       prob NA_share #>    <int>   <char>     <char>      <num>    <num> #> 1:     1  Species  virginica 0.93939394        0 #> 2:     1  Species     setosa 0.03030303        0 #> 3:     1  Species versicolor 0.03030303        0 #> 4:     2  Species  virginica 0.91304348        0 #> 5:     2  Species     setosa 0.04347826        0 #> 6:     2  Species versicolor 0.04347826        0 # Sum probabilities summary(params_iris$cat[, sum(prob), by = .(f_idx, variable)]$V1) #>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.  #>       1       1       1       1       1       1 summary(params_alpha$cat[, sum(prob), by = .(f_idx, variable)]$V1) #>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.  #>       1       1       1       1       1       1 params_iris #> $cnt #> Key: <f_idx, variable> #>        f_idx     variable   min   max       mu      sigma NA_share #>        <int>       <char> <num> <num>    <num>      <num>    <num> #>     1:     1 Petal.Length  -Inf   Inf 5.933333 0.20816660        0 #>     2:     1  Petal.Width  2.45   Inf 2.500000 0.01041469        0 #>     3:     1 Sepal.Length  -Inf   Inf 6.733333 0.45092498        0 #>     4:     1  Sepal.Width  -Inf   Inf 3.400000 0.17320508        0 #>     5:     2 Petal.Length  -Inf   Inf 5.250000 0.21213203        0 #>    ---                                                             #> 10484:  2621  Sepal.Width  3.30  3.45 3.400000 0.02209289        0 #> 10485:  2622 Petal.Length  -Inf  4.20 1.400000 0.10000000        0 #> 10486:  2622  Petal.Width  -Inf  0.25 0.200000 0.03124407        0 #> 10487:  2622 Sepal.Length  4.65  5.55 5.266667 0.20816660        0 #> 10488:  2622  Sepal.Width  3.45  3.55 3.500000 0.02082938        0 #>  #> $cat #> Key: <f_idx, variable> #>       f_idx variable        val      prob NA_share #>       <int>   <char>     <char>     <num>    <num> #>    1:     1  Species  virginica 1.0000000        0 #>    2:     2  Species  virginica 1.0000000        0 #>    3:     3  Species  virginica 1.0000000        0 #>    4:     4  Species  virginica 1.0000000        0 #>    5:     5  Species     setosa 1.0000000        0 #>   ---                                              #> 2919:  2618  Species versicolor 0.4615385        0 #> 2920:  2619  Species     setosa 1.0000000        0 #> 2921:  2620  Species     setosa 1.0000000        0 #> 2922:  2621  Species     setosa 1.0000000        0 #> 2923:  2622  Species     setosa 1.0000000        0 #>  #> $forest #> Key: <tree, leaf> #>       f_idx  tree  leaf        cvg #>       <int> <int> <int>      <num> #>    1:     1     1     6 0.02000000 #>    2:     2     1    10 0.01333333 #>    3:     3     1    13 0.04000000 #>    4:     4     1    16 0.03333333 #>    5:     5     1    21 0.02000000 #>   ---                              #> 2618:  2618   100    49 0.08666667 #> 2619:  2619   100    57 0.02000000 #> 2620:  2620   100    58 0.04666667 #> 2621:  2621   100    60 0.04000000 #> 2622:  2622   100    61 0.02000000 #>  #> $meta #>        variable   class    family decimals #>          <char>  <char>    <char>    <int> #> 1: Sepal.Length numeric truncnorm        1 #> 2:  Sepal.Width numeric truncnorm        1 #> 3: Petal.Length numeric truncnorm        1 #> 4:  Petal.Width numeric truncnorm        1 #> 5:      Species  factor  multinom       NA #>  #> $levels #>    variable        val #>      <char>     <char> #> 1:  Species     setosa #> 2:  Species versicolor #> 3:  Species  virginica #>  #> $input_class #> [1] \"data.frame\""},{"path":"https://bips-hb.github.io/arf/articles/arf.html","id":"likelihood-estimation","dir":"Articles","previous_headings":"","what":"Likelihood Estimation","title":"Adversarial Random Forests","text":"calculate log-likelihoods, pass params lik function, along data whose likelihood wish evaluate. total evidence queries (.e., spanning variables conditioning events), faster also include arf function call. Note piecewise constant estimator considerably worse experiment. lik function can also used compute likelihood partial state, .e.¬†setting variable values specified. Let‚Äôs take look iris dataset: Say want calculate sample likelihoods using continuous data. , provide values first four variables exclude fifth. case, model marginalize Species:  find likelihoods almost identical, slightly higher likelihood average partial samples. expected, since less variation model. example, used data throughout. may lead overfitting. sufficient data, preferable use training set adversarial_rf, validation set forde, test set lik. Alternatively, can set oob argument TRUE either latter two functions, case computations performed --bag (OOB) data. samples randomly excluded given tree due bootstrapping subroutine RF classifier. Note works dataset x passed forde lik one used train arf. Recall sample‚Äôs probability excluded single tree exp(‚àí1)‚âà0.368\\exp(-1) \\approx 0.368. using oob = TRUE, sure include enough trees every observation likely OOB least times.","code":"# Compute likelihood under truncated normal and uniform distributions ll <- lik(params_iris, iris, arf = arf_iris) ll_unif <- lik(params_unif, iris, arf = arf_iris)  # Compare average negative log-likelihood (lower is better) -mean(ll) #> [1] 0.3196718 -mean(ll_unif) #> [1] -7.291452 head(iris) #>   Sepal.Length Sepal.Width Petal.Length Petal.Width Species #> 1          5.1         3.5          1.4         0.2  setosa #> 2          4.9         3.0          1.4         0.2  setosa #> 3          4.7         3.2          1.3         0.2  setosa #> 4          4.6         3.1          1.5         0.2  setosa #> 5          5.0         3.6          1.4         0.2  setosa #> 6          5.4         3.9          1.7         0.4  setosa # Compute likelihoods after marginalizing over Species iris_without_species <- iris[, -5] ll_cnt <- lik(params_iris, iris_without_species)  # Compare results tmp <- data.frame(Total = ll, Partial = ll_cnt) ggplot(tmp, aes(Total, Partial)) +    geom_point() +    geom_abline(slope = 1, intercept = 0, color = 'red')"},{"path":"https://bips-hb.github.io/arf/articles/arf.html","id":"data-synthesis","dir":"Articles","previous_headings":"","what":"Data Synthesis","title":"Adversarial Random Forests","text":"experiment, use smiley simulation mlbench package, allows easy visual assessment. draw training set n=1000n = 1000 simulate 10001000 synthetic datapoints. Resulting data plotted side side.  general shape clearly recognizable, even stray samples evident borders always crisp. can improved training data.","code":"# Simulate training data library(mlbench) x <- mlbench.smiley(1000) x <- data.frame(x$x, x$classes) colnames(x) <- c('X', 'Y', 'Class')  # Fit ARF arf_smiley <- adversarial_rf(x, mtry = 2) #> Iteration: 0, Accuracy: 84.35% #> Iteration: 1, Accuracy: 39.5%  # Estimate parameters params_smiley <- forde(arf_smiley, x)  # Simulate data synth <- forge(params_smiley, n_synth = 1000)  # Compare structure str(x) #> 'data.frame':    1000 obs. of  3 variables: #>  $ X    : num  -0.841 -0.911 -0.91 -0.743 -0.863 ... #>  $ Y    : num  0.874 0.926 1.051 0.918 1.157 ... #>  $ Class: Factor w/ 4 levels \"1\",\"2\",\"3\",\"4\": 1 1 1 1 1 1 1 1 1 1 ... str(synth) #> 'data.frame':    1000 obs. of  3 variables: #>  $ X    : num  -0.3742 -0.0897 0.0322 0.0271 -0.8529 ... #>  $ Y    : num  -0.786 0.371 0.548 0.546 -0.247 ... #>  $ Class: Factor w/ 4 levels \"1\",\"2\",\"3\",\"4\": 4 3 3 3 4 4 3 2 3 3 ...  # Put it all together x$Data <- 'Original' synth$Data <- 'Synthetic' df <- rbind(x, synth)  # Plot results ggplot(df, aes(X, Y, color = Class, shape = Class)) +    geom_point() +    facet_wrap(~ Data)"},{"path":"https://bips-hb.github.io/arf/articles/arf.html","id":"conditioning","dir":"Articles","previous_headings":"","what":"Conditioning","title":"Adversarial Random Forests","text":"ARFs can also used compute likelihoods synthesize data conditioning events specify values ranges input features. instance, say want evaluate likelihood samples iris dataset condition Species = 'setosa'. several ways encode evidence, simplest pass partial observation.  expected, measurements non-setosa samples appear much less likely conditioning event. partial observation method passing evidence requires users specify unique feature values conditioning variable. flexible alternative construct data frame conditioning events, potentially including inequalities. example, may want calculate likelihood samples given Species = 'setosa' Petal.Width > 0.3. can also define intervals: Resulting likelihoods computed condition queries drawn setosa flowers petal width interval (0.3,0.5)(0.3, 0.5). final method passing evidence directly compute posterior distribution leaves. useful particularly complex conditioning events currently inbuilt interface, polynomial constraints arbitrary propositions disjunctive normal form. case, just require data frame columns f_idx wt. latter sum unity, distribution normalized warning. methods can used conditional sampling well.  conditioning Class = 4, restrict sampling smile , rather eyes nose. Computing conditional expectations similarly straightforward. average X,YX, Y coordinates smile .","code":"# Compute conditional likelihoods evi <- data.frame(Species = 'setosa') ll_conditional <- lik(params_iris, query = iris_without_species, evidence = evi)  # Compare NLL across species (shifting to positive range for visualization) tmp <- iris tmp$NLL <- -ll_conditional + max(ll_conditional) + 1 ggplot(tmp, aes(Species, NLL, fill = Species)) +    geom_boxplot() +    scale_y_log10() +    ylab('Negative Log-Likelihood') +    theme(legend.position = 'none') #> Warning: Removed 11 rows containing non-finite outside the scale range #> (`stat_boxplot()`). # Data frame of conditioning events evi <- data.frame(Species = \"setosa\",                    Petal.Width = \">0.3\") evi #>   Species Petal.Width #> 1  setosa        >0.3 evi <- data.frame(Species = \"setosa\",                    Petal.Width = \"(0.3,0.5)\") evi #>   Species Petal.Width #> 1  setosa   (0.3,0.5) # Drawing random weights evi <- data.frame(f_idx = params_iris$forest$f_idx,                   wt = rexp(nrow(params_iris$forest))) evi$wt <- evi$wt / sum(evi$wt) head(evi) #>   f_idx           wt #> 1     1 4.483113e-04 #> 2     2 3.997981e-05 #> 3     3 2.888576e-05 #> 4     4 8.726013e-04 #> 5     5 4.433602e-04 #> 6     6 3.536210e-05 # Simulate class-conditional data for smiley example evi <- data.frame(Class = 4) synth2 <- forge(params_smiley, n_synth = 250, evidence = evi)  # Put it all together synth2$Data <- 'Synthetic' df <- rbind(x, synth2)  # Plot results ggplot(df, aes(X, Y, color = Class, shape = Class)) +    geom_point() +    facet_wrap(~ Data) expct(params_smiley, evidence = evi) #>             X          Y #> 1 -0.01177414 -0.6728099"},{"path":"https://bips-hb.github.io/arf/articles/arf.html","id":"data-imputation","dir":"Articles","previous_headings":"","what":"Data imputation","title":"Adversarial Random Forests","text":"ARFs can used impute missing values datasets. can done either one-shot basis, case plug (conditional) expectation feature(s) missing values; multiple imputation routine, case sample values relevant (conditional) distribution. instance, consider palmer penguins dataset, includes several missing values. can fill NA‚Äôs row 4 expected value, conditional non-NA values (species island) follows: Alternatively, can perform multiple imputation get better sense range plausible values conditional distributions.","code":"# Load palmer penguins dataset library(palmerpenguins) head(penguins) #> # A tibble: 6 √ó 8 #>   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g #>   <fct>   <fct>              <dbl>         <dbl>             <int>       <int> #> 1 Adelie  Torgersen           39.1          18.7               181        3750 #> 2 Adelie  Torgersen           39.5          17.4               186        3800 #> 3 Adelie  Torgersen           40.3          18                 195        3250 #> 4 Adelie  Torgersen           NA            NA                  NA          NA #> 5 Adelie  Torgersen           36.7          19.3               193        3450 #> 6 Adelie  Torgersen           39.3          20.6               190        3650 #> # ‚Ñπ 2 more variables: sex <fct>, year <int> # Single imputation penguins_imp <- impute(penguins) head(penguins_imp) #> # A tibble: 6 √ó 8 #>   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g #>   <fct>   <fct>              <dbl>         <dbl>             <int>       <int> #> 1 Adelie  Torgersen           39.1          18.7               181        3750 #> 2 Adelie  Torgersen           39.5          17.4               186        3800 #> 3 Adelie  Torgersen           40.3          18                 195        3250 #> 4 Adelie  Torgersen           39            18.5               189        3717 #> 5 Adelie  Torgersen           36.7          19.3               193        3450 #> 6 Adelie  Torgersen           39.3          20.6               190        3650 #> # ‚Ñπ 2 more variables: sex <fct>, year <int> # Multiple imputation penguins_multiple_imp <- impute(penguins, m = 100)  # Check the distribution of bill_length_mm for sample 4 tmp <- sapply(penguins_multiple_imp, function(dat) dat$bill_length_mm[4]) hist(tmp, breaks = 20)"},{"path":"https://bips-hb.github.io/arf/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Marvin N. Wright. Author, maintainer. David S. Watson. Author. Kristin Blesch. Author. Jan Kapar. Author.","code":""},{"path":"https://bips-hb.github.io/arf/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Watson D, Blesch K, Kapar J, Wright M (2023). ‚ÄúAdversarial random forests density estimation generative modeling.‚Äù Proceedings 26th International Conference Artificial Intelligence Statistics (AISTATS), volume 206, 5357-5375.","code":"@InProceedings{,   title = {Adversarial random forests for density estimation and generative modeling},   author = {David S. Watson and Kristin Blesch and Jan Kapar and Marvin N. Wright},   booktitle = {Proceedings of The 26th International Conference on Artificial Intelligence and Statistics (AISTATS)},   year = {2023},   publisher = {PMLR},   volume = {206},   pages = {5357-5375}, }"},{"path":[]},{"path":"https://bips-hb.github.io/arf/index.html","id":"introduction","dir":"","previous_headings":"","what":"Introduction","title":"Adversarial Random Forests","text":"Adversarial random forests (ARFs) recursively partition data fully factorized leaves, features jointly independent. procedure iterative, alternating rounds generation discrimination. Data become increasingly realistic round, original synthetic samples can longer reliably distinguished. useful several unsupervised learning tasks, density estimation data synthesis. Methods implemented package. ARFs naturally handle unstructured data mixed continuous categorical covariates. inherit many benefits RFs, including speed, flexibility, solid performance default parameters.","code":""},{"path":"https://bips-hb.github.io/arf/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Adversarial Random Forests","text":"arf package available CRAN: install development version GitHub using devtools, run:","code":"install.packages(\"arf\") devtools::install_github(\"bips-hb/arf\")"},{"path":"https://bips-hb.github.io/arf/index.html","id":"examples","dir":"","previous_headings":"","what":"Examples","title":"Adversarial Random Forests","text":"Using Fisher‚Äôs iris dataset, train ARF estimate distribution parameters:","code":"# Train the ARF arf <- adversarial_rf(iris)  # Estimate distribution parameters psi <- forde(arf, iris)"},{"path":"https://bips-hb.github.io/arf/index.html","id":"density-estimation","dir":"","previous_headings":"Examples","what":"Density estimation","title":"Adversarial Random Forests","text":"estimate log-likelihoods:","code":"mean(lik(arf, psi, iris))"},{"path":"https://bips-hb.github.io/arf/index.html","id":"generative-modeling","dir":"","previous_headings":"Examples","what":"Generative modeling","title":"Adversarial Random Forests","text":"generate 100 synthetic samples:","code":"forge(psi, 100)"},{"path":"https://bips-hb.github.io/arf/index.html","id":"conditional-expectations","dir":"","previous_headings":"Examples","what":"Conditional expectations","title":"Adversarial Random Forests","text":"estimate mean variable(s), optionally conditioned event(s): detailed examples, see package vignette.","code":"evi <- data.frame(Species = \"setosa\") expct(psi, query = \"Sepal.Length\", evidence = evi)"},{"path":"https://bips-hb.github.io/arf/index.html","id":"python-library","dir":"","previous_headings":"","what":"Python library","title":"Adversarial Random Forests","text":"Python implementation ARF, arfpy, available PyPI. development version, see .","code":""},{"path":"https://bips-hb.github.io/arf/index.html","id":"references","dir":"","previous_headings":"","what":"References","title":"Adversarial Random Forests","text":"Watson, D. S., Blesch, K., Kapar, J. & Wright, M. N. (2023). Adversarial random forests density estimation generative modeling. Proceedings 26th International Conference Artificial Intelligence Statistics. Link . Blesch, K., Koenen, N., Kapar, J., Golchian, P., Burk, L., Loecher, M. & Wright, M. N. (2025). Conditional feature importance generative modeling using adversarial random forests. Proceedings 39th AAAI Conference Artificial Intelligence. Link .","code":""},{"path":"https://bips-hb.github.io/arf/reference/adversarial_rf.html","id":null,"dir":"Reference","previous_headings":"","what":"Adversarial Random Forests ‚Äî adversarial_rf","title":"Adversarial Random Forests ‚Äî adversarial_rf","text":"Implements adversarial random forest learn independence-inducing splits.","code":""},{"path":"https://bips-hb.github.io/arf/reference/adversarial_rf.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Adversarial Random Forests ‚Äî adversarial_rf","text":"","code":"adversarial_rf(   x,   num_trees = 10L,   min_node_size = 2L,   delta = 0,   max_iters = 10L,   early_stop = TRUE,   prune = TRUE,   verbose = TRUE,   parallel = TRUE,   ... )"},{"path":"https://bips-hb.github.io/arf/reference/adversarial_rf.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Adversarial Random Forests ‚Äî adversarial_rf","text":"x Input data. Integer variables recoded ordered factors warning. See Details. num_trees Number trees grow forest. default works well generative modeling tasks, increased likelihood estimation. See Details. min_node_size Minimal number real data samples leaf nodes. delta Tolerance parameter. Algorithm converges OOB accuracy < 0.5 + delta. max_iters Maximum iterations adversarial loop. early_stop Terminate loop performance fails improve one round next? prune Impose min_node_size pruning? verbose Print discriminator accuracy round? also show additional warnings. parallel Compute parallel? Must register backend beforehand, e.g. via doParallel doFuture; see examples. ... Extra parameters passed ranger.","code":""},{"path":"https://bips-hb.github.io/arf/reference/adversarial_rf.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Adversarial Random Forests ‚Äî adversarial_rf","text":"random forest object class ranger.","code":""},{"path":"https://bips-hb.github.io/arf/reference/adversarial_rf.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Adversarial Random Forests ‚Äî adversarial_rf","text":"adversarial random forest (ARF) algorithm partitions data fully factorized leaves features jointly independent. ARFs trained iteratively, alternating rounds generation discrimination. first instance, synthetic data generated via independent bootstraps feature, RF classifier trained distinguish real fake samples. subsequent rounds, synthetic data generated separately leaf, using splits previous forest. creates increasingly realistic data satisfies local independence construction. algorithm converges RF reliably distinguish two classes, .e. OOB accuracy falls 0.5 + delta. ARFs useful several unsupervised learning tasks, density estimation (see forde) data synthesis (see forge). former, recommend increasing number trees improved performance (typically order 100-1000 depending sample size). Integer variables recoded warning (set verbose = FALSE silence ). Default behavior convert integer variables six unique values numeric, five unique values treated ordered factors. override behavior, explicitly recode integer variables target type prior training. Note: convergence guaranteed finite samples. max_iters argument sets upper bound number training rounds. Similar results may attained increasing delta. Even single round can often give good performance, data strong complex dependencies may require iterations. default early_stop = TRUE, adversarial loop terminates performance improve one round next, case training may pointless.","code":""},{"path":"https://bips-hb.github.io/arf/reference/adversarial_rf.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Adversarial Random Forests ‚Äî adversarial_rf","text":"Watson, D., Blesch, K., Kapar, J., & Wright, M. (2023). Adversarial random forests density estimation generative modeling. Proceedings 26th International Conference Artificial Intelligence Statistics, pp. 5357-5375.","code":""},{"path":[]},{"path":"https://bips-hb.github.io/arf/reference/adversarial_rf.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Adversarial Random Forests ‚Äî adversarial_rf","text":"","code":"# Train ARF and estimate leaf parameters arf <- adversarial_rf(iris) #> Iteration: 0, Accuracy: 70.71% #> Iteration: 1, Accuracy: 35.47% #> Warning: executing %dopar% sequentially: no parallel backend registered psi <- forde(arf, iris)  # Generate 100 synthetic samples from the iris dataset x_synth <- forge(psi, n_synth = 100)  # Condition on Species = \"setosa\" and Sepal.Length > 6 evi <- data.frame(Species = \"setosa\",                   Sepal.Length = \"(6, Inf)\") x_synth <- forge(psi, n_synth = 100, evidence = evi)  # Estimate average log-likelihood ll <- lik(psi, iris, arf = arf, log = TRUE) mean(ll) #> [1] -0.4833021  # Expectation of Sepal.Length for class setosa evi <- data.frame(Species = \"setosa\") expct(psi, query = \"Sepal.Length\", evidence = evi) #>   Sepal.Length #> 1      5.01668  if (FALSE) { # \\dontrun{ # Parallelization with doParallel doParallel::registerDoParallel(cores = 4)  # ... or with doFuture doFuture::registerDoFuture() future::plan(\"multisession\", workers = 4) } # }"},{"path":"https://bips-hb.github.io/arf/reference/arf-package.html","id":null,"dir":"Reference","previous_headings":"","what":"arf: Adversarial Random Forests ‚Äî arf-package","title":"arf: Adversarial Random Forests ‚Äî arf-package","text":"Adversarial random forests (ARFs) recursively partition data fully factorized leaves, features jointly independent. procedure iterative, alternating rounds generation discrimination. Data becomes increasingly realistic round, original synthetic samples can longer reliably distinguished. useful several unsupervised learning tasks, density estimation data synthesis. Methods implemented package. ARFs naturally handle unstructured data mixed continuous categorical covariates. inherit many benefits random forests, including speed, flexibility, solid performance default parameters. details, see Watson et al. (2023) https://proceedings.mlr.press/v206/watson23a.html.","code":""},{"path":[]},{"path":"https://bips-hb.github.io/arf/reference/arf-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"arf: Adversarial Random Forests ‚Äî arf-package","text":"Maintainer: Marvin N. Wright cran@wrig.de (ORCID) Authors: David S. Watson david.s.watson11@gmail.com (ORCID) Kristin Blesch (ORCID) Jan Kapar (ORCID)","code":""},{"path":"https://bips-hb.github.io/arf/reference/arf-package.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"arf: Adversarial Random Forests ‚Äî arf-package","text":"","code":"# Train ARF and estimate leaf parameters arf <- adversarial_rf(iris) #> Iteration: 0, Accuracy: 78.19% #> Iteration: 1, Accuracy: 42.47% psi <- forde(arf, iris)  # Generate 100 synthetic samples from the iris dataset x_synth <- forge(psi, n_synth = 100)  # Condition on Species = \"setosa\" and Sepal.Length > 6 evi <- data.frame(Species = \"setosa\",                   Sepal.Length = \"(6, Inf)\") x_synth <- forge(psi, n_synth = 100, evidence = evi)  # Estimate average log-likelihood ll <- lik(psi, iris, arf = arf, log = TRUE) mean(ll) #> [1] -0.4604258  # Expectation of Sepal.Length for class setosa evi <- data.frame(Species = \"setosa\") expct(psi, query = \"Sepal.Length\", evidence = evi) #>   Sepal.Length #> 1     5.022068  if (FALSE) { # \\dontrun{ # Parallelization with doParallel doParallel::registerDoParallel(cores = 4)  # ... or with doFuture doFuture::registerDoFuture() future::plan(\"multisession\", workers = 4) } # }"},{"path":"https://bips-hb.github.io/arf/reference/cforde.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute conditional circuit parameters ‚Äî cforde","title":"Compute conditional circuit parameters ‚Äî cforde","text":"Compute conditional circuit parameters","code":""},{"path":"https://bips-hb.github.io/arf/reference/cforde.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute conditional circuit parameters ‚Äî cforde","text":"","code":"cforde(   params,   evidence,   row_mode = c(\"separate\", \"or\"),   nomatch = c(\"force\", \"na\"),   verbose = TRUE,   stepsize = 0,   parallel = TRUE )"},{"path":"https://bips-hb.github.io/arf/reference/cforde.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute conditional circuit parameters ‚Äî cforde","text":"params Circuit parameters learned via forde. evidence Data frame conditioning event(s). row_mode Interpretation rows multi-row conditions. nomatch leaf matches condition evidence? Options force sampling random leaf (\"force\") return NA (\"na\"). default \"force\". verbose Show warnings, e.g. leaf matches condition? stepsize Stepsize defining number condition rows handled one step. parallel Compute parallel? Must register backend beforehand, e.g. via doParallel doFuture; see examples.","code":""},{"path":"https://bips-hb.github.io/arf/reference/cforde.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute conditional circuit parameters ‚Äî cforde","text":"List conditions (evidence_input), prepared conditions (evidence_prepped) leaves match conditions evidence continuous data (cnt) categorical data (cat) well leaf info (forest).","code":""},{"path":"https://bips-hb.github.io/arf/reference/col_rename.html","id":null,"dir":"Reference","previous_headings":"","what":"Adaptive column renaming ‚Äî col_rename","title":"Adaptive column renaming ‚Äî col_rename","text":"function renames columns case input colnames includes colnames required internal functions (e.g., \"y\").","code":""},{"path":"https://bips-hb.github.io/arf/reference/col_rename.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Adaptive column renaming ‚Äî col_rename","text":"","code":"col_rename(cn, old_name)"},{"path":"https://bips-hb.github.io/arf/reference/col_rename.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Adaptive column renaming ‚Äî col_rename","text":"cn Column names. old_name Name column renamed.","code":""},{"path":"https://bips-hb.github.io/arf/reference/col_rename_all.html","id":null,"dir":"Reference","previous_headings":"","what":"Rename all problematic columns with col_rename(). ‚Äî col_rename_all","title":"Rename all problematic columns with col_rename(). ‚Äî col_rename_all","text":"Rename problematic columns col_rename().","code":""},{"path":"https://bips-hb.github.io/arf/reference/col_rename_all.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Rename all problematic columns with col_rename(). ‚Äî col_rename_all","text":"","code":"col_rename_all(cn)"},{"path":"https://bips-hb.github.io/arf/reference/col_rename_all.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Rename all problematic columns with col_rename(). ‚Äî col_rename_all","text":"cn Old column names.","code":""},{"path":"https://bips-hb.github.io/arf/reference/col_rename_all.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Rename all problematic columns with col_rename(). ‚Äî col_rename_all","text":"New columns names.","code":""},{"path":"https://bips-hb.github.io/arf/reference/darf.html","id":null,"dir":"Reference","previous_headings":"","what":"Shortcut likelihood function ‚Äî darf","title":"Shortcut likelihood function ‚Äî darf","text":"Calls adversarial_rf, forde lik. repeated application, faster save outputs adversarial_rf forde pass via ... directly use lik.","code":""},{"path":"https://bips-hb.github.io/arf/reference/darf.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Shortcut likelihood function ‚Äî darf","text":"","code":"darf(x, query = NULL, ...)"},{"path":"https://bips-hb.github.io/arf/reference/darf.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Shortcut likelihood function ‚Äî darf","text":"x Input data. Integer variables recoded ordered factors warning. See Details. query Data frame samples, optionally comprising just subset training features. See Details lik. set x zero. ... Extra parameters passed adversarial_rf, forde lik.","code":""},{"path":"https://bips-hb.github.io/arf/reference/darf.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Shortcut likelihood function ‚Äî darf","text":"vector likelihoods, optionally log scale. dataset n_synth synthetic samples nrow(x) synthetic samples n_synth undefined.","code":""},{"path":"https://bips-hb.github.io/arf/reference/darf.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Shortcut likelihood function ‚Äî darf","text":"Watson, D., Blesch, K., Kapar, J., & Wright, M. (2023). Adversarial random forests density estimation generative modeling. Proceedings 26th International Conference Artificial Intelligence Statistics, pp. 5357-5375.","code":""},{"path":[]},{"path":"https://bips-hb.github.io/arf/reference/darf.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Shortcut likelihood function ‚Äî darf","text":"","code":"# Estimate log-likelihoods ll <- darf(iris)  # Partial evidence query ll <- darf(iris, query = iris[1, 1:3])  # Condition on Species = \"setosa\" ll <- darf(iris, query = iris[1, 1:3], evidence = data.frame(Species = \"setosa\"))"},{"path":"https://bips-hb.github.io/arf/reference/earf.html","id":null,"dir":"Reference","previous_headings":"","what":"Shortcut expectation function ‚Äî earf","title":"Shortcut expectation function ‚Äî earf","text":"Calls adversarial_rf, forde expct. repeated application, faster save outputs adversarial_rf forde pass via ... directly use expct.","code":""},{"path":"https://bips-hb.github.io/arf/reference/earf.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Shortcut expectation function ‚Äî earf","text":"","code":"earf(x, ...)"},{"path":"https://bips-hb.github.io/arf/reference/earf.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Shortcut expectation function ‚Äî earf","text":"x Input data. Integer variables recoded ordered factors warning. See Details. ... Extra parameters passed adversarial_rf, forde expct.","code":""},{"path":"https://bips-hb.github.io/arf/reference/earf.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Shortcut expectation function ‚Äî earf","text":"one row data frame values query variables.","code":""},{"path":"https://bips-hb.github.io/arf/reference/earf.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Shortcut expectation function ‚Äî earf","text":"Watson, D., Blesch, K., Kapar, J., & Wright, M. (2023). Adversarial random forests density estimation generative modeling. Proceedings 26th International Conference Artificial Intelligence Statistics, pp. 5357-5375.","code":""},{"path":[]},{"path":"https://bips-hb.github.io/arf/reference/earf.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Shortcut expectation function ‚Äî earf","text":"","code":"# What is the expected values of each feature? earf(iris) #>   Sepal.Length Sepal.Width Petal.Length Petal.Width   Species #> 1     5.843333    3.057333        3.758    1.199333 virginica  #' # What is the expected values of Sepal.Length? earf(iris, query = \"Sepal.Length\") #>   Sepal.Length #> 1     5.843333  # What if we condition on Species = \"setosa\"? earf(iris, query = \"Sepal.Length\", evidence = data.frame(Species = \"setosa\")) #>   Sepal.Length #> 1     5.010467"},{"path":"https://bips-hb.github.io/arf/reference/expct.html","id":null,"dir":"Reference","previous_headings":"","what":"Expected Value ‚Äî expct","title":"Expected Value ‚Äî expct","text":"Compute expectation query variable(s), optionally conditioned event(s).","code":""},{"path":"https://bips-hb.github.io/arf/reference/expct.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Expected Value ‚Äî expct","text":"","code":"expct(   params,   query = NULL,   evidence = NULL,   evidence_row_mode = c(\"separate\", \"or\"),   round = FALSE,   nomatch = c(\"force\", \"na\"),   verbose = TRUE,   stepsize = 0,   parallel = TRUE )"},{"path":"https://bips-hb.github.io/arf/reference/expct.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Expected Value ‚Äî expct","text":"params Circuit parameters learned via forde. query Optional character vector variable names. Estimates computed . NULL, variables evidence estimated. evidence contains NAs, values imputed full dataset returned. evidence Optional set conditioning events. can take one three forms: (1) partial sample, .e. single row data columns; (2) data frame conditioning events, allows inequalities intervals; (3) posterior distribution leaves. See Details Examples. evidence_row_mode Interpretation rows multi-row evidence. \"separate\", row evidence unique conditioning event n_synth synthetic samples generated. \"\", rows combined logical . See Examples. round Round continuous variables respective maximum precision real data set? nomatch leaf matches condition evidence? Options force sampling random leaf (\"force\") return NA (\"na\"). default \"force\". verbose Show warnings, e.g. leaf matches condition? stepsize many rows evidence handled step? Defaults nrow(evidence) / num_registered_workers parallel == TRUE. parallel Compute parallel? Must register backend beforehand, e.g. via doParallel doFuture; see Examples.","code":""},{"path":"https://bips-hb.github.io/arf/reference/expct.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Expected Value ‚Äî expct","text":"one row data frame values query variables.","code":""},{"path":"https://bips-hb.github.io/arf/reference/expct.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Expected Value ‚Äî expct","text":"function computes expected values subset features, optionally conditioned event(s). three methods (optionally) encoding conditioning events via evidence argument. first provide partial sample, columns training data missing set NA. second provide data frame condition events. supports inequalities intervals. Alternatively, users may directly input pre-calculated posterior distribution leaves, columns f_idx wt. may preferable complex constraints. See Examples. Please note results continuous features included query evidence interval condition currently inconsistent.","code":""},{"path":"https://bips-hb.github.io/arf/reference/expct.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Expected Value ‚Äî expct","text":"Watson, D., Blesch, K., Kapar, J., & Wright, M. (2023). Adversarial random forests density estimation generative modeling. Proceedings 26th International Conference Artificial Intelligence Statistics, pp. 5357-5375.","code":""},{"path":[]},{"path":"https://bips-hb.github.io/arf/reference/expct.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Expected Value ‚Äî expct","text":"","code":"# Train ARF and estimate leaf parameters arf <- adversarial_rf(iris) #> Iteration: 0, Accuracy: 77.03% #> Iteration: 1, Accuracy: 33.22% psi <- forde(arf, iris)  # What is the expected value of Sepal.Length? expct(psi, query = \"Sepal.Length\") #>   Sepal.Length #> 1     5.843333  # What if we condition on Species = \"setosa\"? evi <- data.frame(Species = \"setosa\") expct(psi, query = \"Sepal.Length\", evidence = evi) #>   Sepal.Length #> 1     5.026998  # Compute expectations for all features other than Species expct(psi, evidence = evi) #>   Sepal.Length Sepal.Width Petal.Length Petal.Width #> 1     5.026998    3.414044     1.542446   0.2771403  # Condition on Species = \"setosa\" and Petal.Width > 0.3 evi <- data.frame(Species = \"setosa\",                    Petal.Width = \">0.3\") expct(psi, evidence = evi) #>   Sepal.Length Sepal.Width Petal.Length #> 1     5.208278    3.515179     1.755113  # Condition on first two rows with some missing values evi <- iris[1:2,] evi[1, 1] <- NA_real_ evi[1, 5] <- NA_character_ evi[2, 2] <- NA_real_ x_synth <- expct(psi, evidence = evi)  if (FALSE) { # \\dontrun{ # Parallelization with doParallel doParallel::registerDoParallel(cores = 4)  # ... or with doFuture doFuture::registerDoFuture() future::plan(\"multisession\", workers = 4) } # }"},{"path":"https://bips-hb.github.io/arf/reference/forde.html","id":null,"dir":"Reference","previous_headings":"","what":"Forests for Density Estimation ‚Äî forde","title":"Forests for Density Estimation ‚Äî forde","text":"Uses pre-trained ARF model estimate leaf distribution parameters.","code":""},{"path":"https://bips-hb.github.io/arf/reference/forde.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Forests for Density Estimation ‚Äî forde","text":"","code":"forde(   arf,   x,   oob = FALSE,   family = \"truncnorm\",   finite_bounds = c(\"no\", \"local\", \"global\"),   alpha = 0,   epsilon = 0,   parallel = TRUE )"},{"path":"https://bips-hb.github.io/arf/reference/forde.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Forests for Density Estimation ‚Äî forde","text":"arf Pre-trained adversarial_rf. Alternatively, object class ranger. x Training data estimating parameters. oob use --bag samples parameter estimation? TRUE, x must dataset used train arf. Set \"inbag\" use -bag samples. Default FALSE, .e. use observations. family Distribution use density estimation continuous features. Current options include truncated normal (default family = \"truncnorm\") uniform (family = \"unif\"). See Details. finite_bounds Impose finite bounds continuous variables? \"local\", infinite bounds set empirical extrema within leaves. \"global\", infinite bounds set global empirical extrema. \"\" (default), infinite bounds left unchanged. alpha Optional pseudocount Laplace smoothing categorical features. avoids zero-mass points test data fall outside support training data. Effectively parameterizes flat Dirichlet prior multinomial likelihoods. epsilon Optional slack parameter empirical bounds finite_bounds != \"\". avoids zero-density points test data fall outside support training data. gap lower upper bounds expanded factor 1 + epsilon. parallel Compute parallel? Must register backend beforehand, e.g. via doParallel doFuture; see examples.","code":""},{"path":"https://bips-hb.github.io/arf/reference/forde.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Forests for Density Estimation ‚Äî forde","text":"list 5 elements: (1) parameters continuous data; (2) parameters discrete data; (3) leaf indices coverage; (4) metadata variables; (5) data input class. list used estimating likelihoods lik generating data forge.","code":""},{"path":"https://bips-hb.github.io/arf/reference/forde.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Forests for Density Estimation ‚Äî forde","text":"forde extracts leaf parameters pretrained forest learns distribution parameters data within leaf. former includes coverage (proportion data falling leaf) split criteria. latter includes proportions categorical features mean/variance continuous features. result probabilistic circuit, stored data.table, can used various downstream inference tasks. Currently, forde provides support limited number distributional families: truncated normal uniform continuous data, multinomial discrete data. Though forde designed take adversarial random forest input, function's first argument can principle object class ranger. allows users test performance alternative pipelines (e.g., supervised forest input). also requirement x data used fit arf, unless oob = TRUE. fact, using another dataset may protect overfitting. connects Wager & Athey's (2018) notion \"honest trees\".","code":""},{"path":"https://bips-hb.github.io/arf/reference/forde.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Forests for Density Estimation ‚Äî forde","text":"Watson, D., Blesch, K., Kapar, J., & Wright, M. (2023). Adversarial random forests density estimation generative modeling. Proceedings 26th International Conference Artificial Intelligence Statistics, pp. 5357-5375. Wager, S. & Athey, S. (2018). Estimation inference heterogeneous treatment effects using random forests. J. . Stat. Assoc., 113(523): 1228-1242.","code":""},{"path":[]},{"path":"https://bips-hb.github.io/arf/reference/forde.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Forests for Density Estimation ‚Äî forde","text":"","code":"# Train ARF and estimate leaf parameters arf <- adversarial_rf(iris) #> Iteration: 0, Accuracy: 78.52% #> Iteration: 1, Accuracy: 41.16% psi <- forde(arf, iris)  # Generate 100 synthetic samples from the iris dataset x_synth <- forge(psi, n_synth = 100)  # Condition on Species = \"setosa\" and Sepal.Length > 6 evi <- data.frame(Species = \"setosa\",                   Sepal.Length = \"(6, Inf)\") x_synth <- forge(psi, n_synth = 100, evidence = evi)  # Estimate average log-likelihood ll <- lik(psi, iris, arf = arf, log = TRUE) mean(ll) #> [1] -0.4118479  # Expectation of Sepal.Length for class setosa evi <- data.frame(Species = \"setosa\") expct(psi, query = \"Sepal.Length\", evidence = evi) #>   Sepal.Length #> 1      5.01768  if (FALSE) { # \\dontrun{ # Parallelization with doParallel doParallel::registerDoParallel(cores = 4)  # ... or with doFuture doFuture::registerDoFuture() future::plan(\"multisession\", workers = 4) } # }"},{"path":"https://bips-hb.github.io/arf/reference/forge.html","id":null,"dir":"Reference","previous_headings":"","what":"Forests for Generative Modeling ‚Äî forge","title":"Forests for Generative Modeling ‚Äî forge","text":"Uses pre-trained FORDE model simulate synthetic data.","code":""},{"path":"https://bips-hb.github.io/arf/reference/forge.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Forests for Generative Modeling ‚Äî forge","text":"","code":"forge(   params,   n_synth,   evidence = NULL,   evidence_row_mode = c(\"separate\", \"or\"),   round = TRUE,   sample_NAs = FALSE,   nomatch = c(\"force\", \"na\"),   verbose = TRUE,   stepsize = 0,   parallel = TRUE )"},{"path":"https://bips-hb.github.io/arf/reference/forge.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Forests for Generative Modeling ‚Äî forge","text":"params Circuit parameters learned via forde. n_synth Number synthetic samples generate. evidence Optional set conditioning events. can take one three forms: (1) partial sample, .e. single row data columns; (2) data frame conditioning events, allows inequalities; (3) posterior distribution leaves. See Details. evidence_row_mode Interpretation rows multi-row evidence. \"separate\", row evidence unique conditioning event n_synth synthetic samples generated. \"\", rows combined logical . See Examples. round Round continuous variables respective maximum precision real data set? sample_NAs Sample NAs respecting probability missing values original data? nomatch leaf matches condition evidence? Options force sampling random leaf (\"force\") return NA (\"na\"). default \"force\". verbose Show warnings, e.g. leaf matches condition? stepsize many rows evidence handled step? Defaults nrow(evidence) / num_registered_workers parallel == TRUE. parallel Compute parallel? Must register backend beforehand, e.g. via doParallel doFuture; see examples.","code":""},{"path":"https://bips-hb.github.io/arf/reference/forge.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Forests for Generative Modeling ‚Äî forge","text":"dataset n_synth synthetic samples.","code":""},{"path":"https://bips-hb.github.io/arf/reference/forge.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Forests for Generative Modeling ‚Äî forge","text":"forge simulates synthetic dataset n_synth samples. First, leaves sampled proportion either coverage (evidence = NULL) posterior probability. , feature sampled independently within leaf according probability mass density function learned forde. create realistic data long adversarial RF used previous step satisfies local independence criterion. See Watson et al. (2023). three methods (optionally) encoding conditioning events via evidence argument. first provide partial sample, columns training data missing set NA. second provide data frame condition events. supports inequalities intervals. Alternatively, users may directly input pre-calculated posterior distribution leaves, columns f_idx wt. may preferable complex constraints. See Examples.","code":""},{"path":"https://bips-hb.github.io/arf/reference/forge.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Forests for Generative Modeling ‚Äî forge","text":"Watson, D., Blesch, K., Kapar, J., & Wright, M. (2023). Adversarial random forests density estimation generative modeling. Proceedings 26th International Conference Artificial Intelligence Statistics, pp. 5357-5375.","code":""},{"path":[]},{"path":"https://bips-hb.github.io/arf/reference/forge.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Forests for Generative Modeling ‚Äî forge","text":"","code":"# Train ARF and estimate leaf parameters arf <- adversarial_rf(iris) #> Iteration: 0, Accuracy: 76.61% #> Iteration: 1, Accuracy: 38.8% psi <- forde(arf, iris)  # Generate 100 synthetic samples from the iris dataset x_synth <- forge(psi, n_synth = 100)  # Condition on Species = \"setosa\" evi <- data.frame(Species = \"setosa\") x_synth <- forge(psi, n_synth = 100, evidence = evi)  # Condition on Species = \"setosa\" and Sepal.Length > 6 evi <- data.frame(Species = \"setosa\",                   Sepal.Length = \"(6, Inf)\") x_synth <- forge(psi, n_synth = 100, evidence = evi)  # Alternative syntax for <\/> conditions evi <- data.frame(Sepal.Length = \">6\") x_synth <- forge(psi, n_synth = 100, evidence = evi)  # Negation condition, i.e. all classes except \"setosa\" evi <- data.frame(Species = \"!setosa\") x_synth <- forge(psi, n_synth = 100, evidence = evi)  # Condition on first two data rows with some missing values evi <- iris[1:2,] evi[1, 1] <- NA_real_ evi[1, 5] <- NA_character_ evi[2, 2] <- NA_real_ x_synth <- forge(psi, n_synth = 1, evidence = evi)  # Or just input some distribution on leaves # (Weights that do not sum to unity are automatically scaled) n_leaves <- nrow(psi$forest) evi <- data.frame(f_idx = psi$forest$f_idx, wt = rexp(n_leaves)) x_synth <- forge(psi, n_synth = 100, evidence = evi)  if (FALSE) { # \\dontrun{ # Parallelization with doParallel doParallel::registerDoParallel(cores = 4)  # ... or with doFuture doFuture::registerDoFuture() future::plan(\"multisession\", workers = 4) } # }"},{"path":"https://bips-hb.github.io/arf/reference/impute.html","id":null,"dir":"Reference","previous_headings":"","what":"Missing value imputation with ARF ‚Äî impute","title":"Missing value imputation with ARF ‚Äî impute","text":"Perform single multiple imputation ARFs. Calls adversarial_rf, forde expct/forge.","code":""},{"path":"https://bips-hb.github.io/arf/reference/impute.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Missing value imputation with ARF ‚Äî impute","text":"","code":"impute(   x,   m = 1,   expectation = ifelse(m == 1, TRUE, FALSE),   num_trees = 100L,   min_node_size = 10L,   round = TRUE,   finite_bounds = \"local\",   epsilon = 1e-14,   verbose = FALSE,   ... )"},{"path":"https://bips-hb.github.io/arf/reference/impute.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Missing value imputation with ARF ‚Äî impute","text":"x Input data. m Number imputed datasets generate. default single imputation (m = 1). expectation Return expected value instead multiple imputations. default, single imputation (m = 1), expected value returned. num_trees Number trees grow ARF. min_node_size Minimal number real data samples leaf nodes. round Round continuous variables respective maximum precision real data set? finite_bounds Impose finite bounds continuous variables? See forde. epsilon Slack parameter empirical bounds; see forde. verbose Print progress adversarial_rf? ... Extra parameters passed adversarial_rf, forde expct/forge.","code":""},{"path":"https://bips-hb.github.io/arf/reference/impute.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Missing value imputation with ARF ‚Äî impute","text":"Imputed data. single dataset returned m = 1, list datasets m > 1.","code":""},{"path":[]},{"path":"https://bips-hb.github.io/arf/reference/impute.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Missing value imputation with ARF ‚Äî impute","text":"","code":"# Generate some missings iris_na <- iris for (j in 1:ncol(iris)) {   iris_na[sample(1:nrow(iris), 5), j] <- NA }  # Single imputation iris_imputed <- arf::impute(iris_na, num_trees = 10, m = 1)  # Multiple imputation iris_imputed <- arf::impute(iris_na, num_trees = 10, m = 10)  if (FALSE) { # \\dontrun{ # Parallelization with doParallel doParallel::registerDoParallel(cores = 4)  # ... or with doFuture doFuture::registerDoFuture() future::plan(\"multisession\", workers = 4) } # }"},{"path":"https://bips-hb.github.io/arf/reference/lik.html","id":null,"dir":"Reference","previous_headings":"","what":"Likelihood Estimation ‚Äî lik","title":"Likelihood Estimation ‚Äî lik","text":"Compute likelihood input data, optionally conditioned event(s).","code":""},{"path":"https://bips-hb.github.io/arf/reference/lik.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Likelihood Estimation ‚Äî lik","text":"","code":"lik(   params,   query,   evidence = NULL,   arf = NULL,   oob = FALSE,   log = TRUE,   batch = NULL,   parallel = TRUE )"},{"path":"https://bips-hb.github.io/arf/reference/lik.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Likelihood Estimation ‚Äî lik","text":"params Circuit parameters learned via forde. query Data frame samples, optionally comprising just subset training features. Likelihoods computed sample. Missing features marginalized . See Details. evidence Optional set conditioning events. can take one three forms: (1) partial sample, .e. single row data columns; (2) data frame conditioning events, allows inequalities; (3) posterior distribution leaves. See Details. arf Pre-trained adversarial_rf object class ranger. required speeds computation considerably total evidence queries. (Ignored partial evidence queries.) oob use --bag leaves likelihood estimation? TRUE, x must dataset used train arf. applicable total evidence queries. log Return likelihoods log scale? Recommended prevent underflow. batch Batch size. default compute densities queries one round, always fastest option memory allows. However, large samples many trees, can memory efficient split data batches. impact results. parallel Compute parallel? Must register backend beforehand, e.g. via doParallel doFuture; see examples.","code":""},{"path":"https://bips-hb.github.io/arf/reference/lik.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Likelihood Estimation ‚Äî lik","text":"vector likelihoods, optionally log scale.","code":""},{"path":"https://bips-hb.github.io/arf/reference/lik.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Likelihood Estimation ‚Äî lik","text":"function computes likelihood input data, optionally conditioned event(s). Queries may partial, .e. covering features, case excluded variables marginalized . three methods (optionally) encoding conditioning events via evidence argument. first provide partial sample, columns training data present. second provide data frame three columns: variable, relation, value. supports inequalities via relation. Alternatively, users may directly input pre-calculated posterior distribution leaves, columns f_idx wt. may preferable complex constraints. See Examples.","code":""},{"path":"https://bips-hb.github.io/arf/reference/lik.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Likelihood Estimation ‚Äî lik","text":"Watson, D., Blesch, K., Kapar, J., & Wright, M. (2023). Adversarial random forests density estimation generative modeling. Proceedings 26th International Conference Artificial Intelligence Statistics, pp. 5357-5375.","code":""},{"path":[]},{"path":"https://bips-hb.github.io/arf/reference/lik.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Likelihood Estimation ‚Äî lik","text":"","code":"# Train ARF and estimate leaf parameters arf <- adversarial_rf(iris) #> Iteration: 0, Accuracy: 76.51% #> Iteration: 1, Accuracy: 34.46% psi <- forde(arf, iris)  # Estimate average log-likelihood ll <- lik(psi, iris, arf = arf, log = TRUE) mean(ll) #> [1] -0.4735089  # Identical but slower ll <- lik(psi, iris, log = TRUE) #> Warning: For total evidence queries, it is faster to include the pre-trained arf. mean(ll) #> [1] -0.4735089  # Partial evidence query lik(psi, query = iris[1, 1:3]) #> [1] 0.7540301  # Condition on Species = \"setosa\" evi <- data.frame(Species = \"setosa\") lik(psi, query = iris[1, 1:3], evidence = evi) #> [1] 1.85244  # Condition on Species = \"setosa\" and Petal.Width > 0.3 evi <- data.frame(Species = \"setosa\",                    Petal.Width = \">0.3\") lik(psi, query = iris[1, 1:3], evidence = evi) #> [1] 1.978245  if (FALSE) { # \\dontrun{ # Parallelization with doParallel doParallel::registerDoParallel(cores = 4)  # ... or with doFuture doFuture::registerDoFuture() future::plan(\"multisession\", workers = 4) } # }"},{"path":"https://bips-hb.github.io/arf/reference/post_x.html","id":null,"dir":"Reference","previous_headings":"","what":"Post-process data ‚Äî post_x","title":"Post-process data ‚Äî post_x","text":"function prepares output data forge.","code":""},{"path":"https://bips-hb.github.io/arf/reference/post_x.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Post-process data ‚Äî post_x","text":"","code":"post_x(x, params, round = TRUE)"},{"path":"https://bips-hb.github.io/arf/reference/post_x.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Post-process data ‚Äî post_x","text":"x Input data.frame. params Circuit parameters learned via forde. round Round continuous variables respective maximum precision real data set?","code":""},{"path":"https://bips-hb.github.io/arf/reference/prep_cond.html","id":null,"dir":"Reference","previous_headings":"","what":"Preprocess conditions ‚Äî prep_cond","title":"Preprocess conditions ‚Äî prep_cond","text":"Preprocess conditions","code":""},{"path":"https://bips-hb.github.io/arf/reference/prep_cond.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Preprocess conditions ‚Äî prep_cond","text":"","code":"prep_cond(evidence, params, row_mode)"},{"path":"https://bips-hb.github.io/arf/reference/prep_cond.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Preprocess conditions ‚Äî prep_cond","text":"evidence Optional set conditioning events. params Circuit parameters learned via forde. row_mode Interpretation rows multi-row conditions.","code":""},{"path":"https://bips-hb.github.io/arf/reference/prep_x.html","id":null,"dir":"Reference","previous_headings":"","what":"Preprocess input data ‚Äî prep_x","title":"Preprocess input data ‚Äî prep_x","text":"function prepares input data ARFs.","code":""},{"path":"https://bips-hb.github.io/arf/reference/prep_x.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Preprocess input data ‚Äî prep_x","text":"","code":"prep_x(x, verbose = TRUE)"},{"path":"https://bips-hb.github.io/arf/reference/prep_x.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Preprocess input data ‚Äî prep_x","text":"x Input data.frame. verbose Show warning recoding integers?","code":""},{"path":"https://bips-hb.github.io/arf/reference/rarf.html","id":null,"dir":"Reference","previous_headings":"","what":"Shortcut sampling function ‚Äî rarf","title":"Shortcut sampling function ‚Äî rarf","text":"Calls adversarial_rf, forde forge. repeated application, faster save outputs adversarial_rf forde pass via ... directly use forge.","code":""},{"path":"https://bips-hb.github.io/arf/reference/rarf.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Shortcut sampling function ‚Äî rarf","text":"","code":"rarf(x, n_synth = NULL, ...)"},{"path":"https://bips-hb.github.io/arf/reference/rarf.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Shortcut sampling function ‚Äî rarf","text":"x Input data. Integer variables recoded ordered factors warning. See Details. n_synth Number synthetic samples generate unconditional generation evidence given. Number synthetic samples generate per evidence row evidence provided. NULL, defaults nrow(x) evidence provided 1 otherwise. ... Extra parameters passed adversarial_rf, forde forge.","code":""},{"path":"https://bips-hb.github.io/arf/reference/rarf.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Shortcut sampling function ‚Äî rarf","text":"dataset n_synth synthetic samples nrow(x) synthetic samples n_synth undefined.","code":""},{"path":"https://bips-hb.github.io/arf/reference/rarf.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Shortcut sampling function ‚Äî rarf","text":"Watson, D., Blesch, K., Kapar, J., & Wright, M. (2023). Adversarial random forests density estimation generative modeling. Proceedings 26th International Conference Artificial Intelligence Statistics, pp. 5357-5375.","code":""},{"path":[]},{"path":"https://bips-hb.github.io/arf/reference/rarf.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Shortcut sampling function ‚Äî rarf","text":"","code":"# Generate 150 (size of original iris dataset) synthetic samples from the iris dataset x_synth <- rarf(iris)  # Generate 100 synthetic samples from the iris dataset x_synth <- rarf(iris, n_synth = 100)  # Condition on Species = \"setosa\" x_synth <- rarf(iris, evidence = data.frame(Species = \"setosa\"))"},{"path":"https://bips-hb.github.io/arf/reference/resample.html","id":null,"dir":"Reference","previous_headings":"","what":"Safer version of sample() ‚Äî resample","title":"Safer version of sample() ‚Äî resample","text":"Safer version sample()","code":""},{"path":"https://bips-hb.github.io/arf/reference/resample.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Safer version of sample() ‚Äî resample","text":"","code":"resample(x, ...)"},{"path":"https://bips-hb.github.io/arf/reference/resample.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Safer version of sample() ‚Äî resample","text":"x vector one elements choose. ... arguments sample().","code":""},{"path":"https://bips-hb.github.io/arf/reference/resample.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Safer version of sample() ‚Äî resample","text":"vector length size elements drawn x.","code":""},{"path":"https://bips-hb.github.io/arf/reference/which.max.random.html","id":null,"dir":"Reference","previous_headings":"","what":"which.max() with random at ties ‚Äî which.max.random","title":"which.max() with random at ties ‚Äî which.max.random","text":".max() random ties","code":""},{"path":"https://bips-hb.github.io/arf/reference/which.max.random.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"which.max() with random at ties ‚Äî which.max.random","text":"","code":"which.max.random(x)"},{"path":"https://bips-hb.github.io/arf/reference/which.max.random.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"which.max() with random at ties ‚Äî which.max.random","text":"x numeric vector.","code":""},{"path":"https://bips-hb.github.io/arf/reference/which.max.random.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"which.max() with random at ties ‚Äî which.max.random","text":"Index maximum value x, random tie-breaking.","code":""},{"path":"https://bips-hb.github.io/arf/news/index.html","id":"arf-024","dir":"Changelog","previous_headings":"","what":"arf 0.2.4","title":"arf 0.2.4","text":"CRAN release: 2025-02-24 Let verbose=FALSE silence () warnings","code":""},{"path":"https://bips-hb.github.io/arf/news/index.html","id":"arf-023","dir":"Changelog","previous_headings":"","what":"arf 0.2.3","title":"arf 0.2.3","text":"Add impute() function direct missing data imputation ARF Add one-line functions darf(), earf(), rarf()","code":""},{"path":"https://bips-hb.github.io/arf/news/index.html","id":"arf-022","dir":"Changelog","previous_headings":"","what":"arf 0.2.2","title":"arf 0.2.2","text":"Faster vectorized conditional sampling Use min.bucket argument ranger avoid pruning possible Option sample NAs generated data original data contains NAs Stepsize forge() reduce memory usage Option local global finite bounds","code":""},{"path":"https://bips-hb.github.io/arf/news/index.html","id":"arf-020","dir":"Changelog","previous_headings":"","what":"arf 0.2.0","title":"arf 0.2.0","text":"CRAN release: 2024-01-24 Vectorized adversarial resampling Speed boost compiling probabilistic circuit Conditional densities sampling Bayesian solution invariant continuous data within leaf nodes New function computing (conditional) expectations Options missing data","code":""},{"path":"https://bips-hb.github.io/arf/news/index.html","id":"arf-013","dir":"Changelog","previous_headings":"","what":"arf 0.1.3","title":"arf 0.1.3","text":"CRAN release: 2023-02-06 Speed boost adversarial resampling step Early stopping option adversarial training alpha parameter regularizing multinomial distributions forde Unified treatment colnames internal semantics (y, obs, tree, leaf)","code":""}]
