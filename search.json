[{"path":"https://bips-hb.github.io/arf/articles/arf.html","id":"adversarial-training","dir":"Articles","previous_headings":"","what":"Adversarial Training","title":"Adversarial Random Forests","text":"ARF algorithm iterative procedure. first instance, generate synthetic data independently sampling marginals feature training random forest (RF) distinguish original synthetic samples. accuracy greater \\(0.5 + \\delta\\) (delta user-controlled tolerance parameter, generally set 0), create new dataset sampling marginals within leaf training another RF classifier. procedure repeats original synthetic samples reliably distinguished. default verbose = TRUE, algorithm print accuracy iteration. printouts can turned setting verbose = FALSE. Accuracy still stored within arf object, can evaluate convergence fact. warning appears just per session. can suppressed setting parallel = FALSE registering parallel backend ().  find quick drop accuracy following resampling procedure, desired. ARF converged, resulting splits form fully factorized leaves, .e. subregions feature space variables locally independent. ARF convergence asymptotically guaranteed \\(n \\rightarrow \\infty\\) (see Watson et al., 2023, Thm. 1). However, implications finite sample performance. practice, often find adversarial training completes just one two rounds, may hold datasets. avoid infinite loops, users can increase slack parameter delta set max_iters argument (default = 10). addition failsafes, adversarial_rf uses early stopping default (early_stop = TRUE), terminates training factorization improve one round next. recommended, since discriminator accuracy rarely falls much lower increased. density estimation tasks, recommend increasing default number trees. generally use 100 experiments, though may suboptimal datasets. Likelihood estimates sensitive parameter certain threshold, larger models incur extra costs time memory. can speed computations registering parallel backend, case ARF training distributed across cores using ranger package. Much like ranger, default behavior adversarial_rf compute parallel possible. exactly done varies across operating systems. following code works Unix machines. Windows requires different setup. either case, can now execute parallel. result object class ranger, can input downstream functions.","code":"# Load libraries library(arf) library(data.table) library(ggplot2)  # Set seed set.seed(123, \"L'Ecuyer-CMRG\")  # Train ARF arf_iris <- adversarial_rf(iris) #> Iteration: 0, Accuracy: 85.52% #> Iteration: 1, Accuracy: 42.33% #> Warning: executing %dopar% sequentially: no parallel backend registered # Train ARF with no printouts arf_iris <- adversarial_rf(iris, verbose = FALSE)  # Plot accuracy against iterations (model converges when accuracy <= 0.5) tmp <- data.frame('Accuracy' = arf_iris$acc,                    'Iteration' = seq_len(length(arf_iris$acc))) ggplot(tmp, aes(Iteration, Accuracy)) +    geom_point() +    geom_path() +   geom_hline(yintercept = 0.5, linetype = 'dashed', color = 'red') # Register cores - Unix library(doParallel) registerDoParallel(cores = 2) # Register cores - Windows library(doParallel) cl <- makeCluster(2) registerDoParallel(cl) # Rerun ARF, now in parallel and with more trees arf_iris <- adversarial_rf(iris, num_trees = 100) #> Iteration: 0, Accuracy: 91.33% #> Iteration: 1, Accuracy: 33%"},{"path":"https://bips-hb.github.io/arf/articles/arf.html","id":"parameter-learning","dir":"Articles","previous_headings":"","what":"Parameter Learning","title":"Adversarial Random Forests","text":"next step learn leaf distribution parameters using forests density estimation (FORDE). function calculates coverage, bounds, pdf/pmf parameters every variable every leaf. can expensive computation large datasets, requires \\(\\mathcal{O}\\big(B \\cdot d \\cdot n \\cdot \\log(n)\\big)\\) operations, \\(B\\) number trees, \\(d\\) data dimensionality, \\(n\\) sample size. , process parallelized default. Default behavior use truncated normal distribution continuous data (boundaries given tree’s split parameters) multinomial distribution categorical data. find produces stable results wide range settings. can also use uniform distribution continuous features setting family = 'unif', thereby instantiating piecewise constant density estimator. method tends perform poorly practice, recommend . option implemented primarily benchmarking purposes. Alternative families, e.g. truncated Poisson beta distributions, may useful certain problems. Future releases expand range options family argument. alpha epsilon arguments allow optional regularization multinomial uniform distributions, respectively. help prevent zero likelihood samples test data fall outside support training data. former pseudocount parameter applies Laplace smoothing within leaves, preventing unobserved values assigned zero probability unless splits explicitly rule . words, impose flat Dirichlet prior report posterior probabilities rather maximum likelihood estimates. latter slack parameter empirical bounds expands estimated extrema continuous features factor \\(1 + \\epsilon\\). Compare results original probability estimates Species variable obtained adding pseudocount \\(\\alpha = 0.1\\). Laplace smoothing, extreme probabilities occur splits explicitly demand . Otherwise, values shrink toward uniform prior. Note two data tables may exactly rows, omit zero probability events conserve memory. However, can verify probabilities sum unity leaf-variable combination. forde function outputs list length 6, entries (1) continuous features; (2) categorical features; (3) leaf parameters; (4) variable metadata; (5) factor levels; (6) data input class. parameters can used variety downstream tasks, likelihood estimation data synthesis.","code":"# Compute leaf and distribution parameters params_iris <- forde(arf_iris, iris) # Recompute with uniform density params_unif <- forde(arf_iris, iris, family = 'unif') #> Warning in forde(arf_iris, iris, family = \"unif\"): Denisity estimation with #> uniform distribution requires finite bounds. finite_bounds has been set to #> 'local'. # Recompute with additive smoothing params_alpha <- forde(arf_iris, iris, alpha = 0.1)  # Compare results head(params_iris$cat) #> Key: <f_idx, variable> #>    f_idx variable       val  prob NA_share #>    <int>   <char>    <char> <num>    <num> #> 1:     1  Species virginica     1        0 #> 2:     2  Species    setosa     1        0 #> 3:     3  Species    setosa     1        0 #> 4:     4  Species virginica     1        0 #> 5:     5  Species    setosa     1        0 #> 6:     6  Species    setosa     1        0 head(params_alpha$cat) #> Key: <f_idx, variable> #>    f_idx variable        val       prob NA_share #>    <int>   <char>     <char>      <num>    <num> #> 1:     1  Species  virginica 0.93939394        0 #> 2:     1  Species     setosa 0.03030303        0 #> 3:     1  Species versicolor 0.03030303        0 #> 4:     2  Species  virginica 0.03030303        0 #> 5:     2  Species     setosa 0.93939394        0 #> 6:     2  Species versicolor 0.03030303        0 # Sum probabilities summary(params_iris$cat[, sum(prob), by = .(f_idx, variable)]$V1) #>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.  #>       1       1       1       1       1       1 summary(params_alpha$cat[, sum(prob), by = .(f_idx, variable)]$V1) #>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.  #>       1       1       1       1       1       1 params_iris #> $cnt #> Key: <f_idx, variable> #>       f_idx     variable   min   max       mu      sigma NA_share #>       <int>       <char> <num> <num>    <num>      <num>    <num> #>    1:     1 Petal.Length  -Inf   Inf 5.933333 0.20816660        0 #>    2:     1  Petal.Width  2.45   Inf 2.500000 0.01041469        0 #>    3:     1 Sepal.Length  -Inf   Inf 6.733333 0.45092498        0 #>    4:     1  Sepal.Width  -Inf   Inf 3.400000 0.17320508        0 #>    5:     2 Petal.Length  -Inf   Inf 1.466667 0.05773503        0 #>   ---                                                             #> 8468:  2117  Sepal.Width  3.30  3.45 3.400000 0.02209289        0 #> 8469:  2118 Petal.Length  1.35  1.45 1.400000 0.01803877        0 #> 8470:  2118  Petal.Width  -Inf  0.50 0.200000 0.08164966        0 #> 8471:  2118 Sepal.Length  4.65  5.30 4.875000 0.09574271        0 #> 8472:  2118  Sepal.Width  2.45  3.30 3.075000 0.15000000        0 #>  #> $cat #> Key: <f_idx, variable> #>       f_idx variable        val  prob NA_share #>       <int>   <char>     <char> <num>    <num> #>    1:     1  Species  virginica     1        0 #>    2:     2  Species     setosa     1        0 #>    3:     3  Species     setosa     1        0 #>    4:     4  Species  virginica     1        0 #>    5:     5  Species     setosa     1        0 #>   ---                                          #> 2408:  2114  Species versicolor     1        0 #> 2409:  2115  Species     setosa     1        0 #> 2410:  2116  Species     setosa     1        0 #> 2411:  2117  Species     setosa     1        0 #> 2412:  2118  Species     setosa     1        0 #>  #> $forest #> Key: <tree, leaf> #>       f_idx  tree  leaf        cvg #>       <int> <int> <int>      <num> #>    1:     1     1     6 0.02000000 #>    2:     2     1    11 0.02000000 #>    3:     3     1    15 0.02000000 #>    4:     4     1    25 0.05333333 #>    5:     5     1    31 0.01333333 #>   ---                              #> 2114:  2114   100    95 0.03333333 #> 2115:  2115   100    97 0.01333333 #> 2116:  2116   100   101 0.04000000 #> 2117:  2117   100   103 0.04000000 #> 2118:  2118   100   105 0.02666667 #>  #> $meta #>        variable   class    family decimals #>          <char>  <char>    <char>    <int> #> 1: Sepal.Length numeric truncnorm        1 #> 2:  Sepal.Width numeric truncnorm        1 #> 3: Petal.Length numeric truncnorm        1 #> 4:  Petal.Width numeric truncnorm        1 #> 5:      Species  factor  multinom       NA #>  #> $levels #>    variable        val #>      <fctr>     <char> #> 1:  Species  virginica #> 2:  Species     setosa #> 3:  Species versicolor #>  #> $input_class #> [1] \"data.frame\""},{"path":"https://bips-hb.github.io/arf/articles/arf.html","id":"likelihood-estimation","dir":"Articles","previous_headings":"","what":"Likelihood Estimation","title":"Adversarial Random Forests","text":"calculate log-likelihoods, pass params lik function, along data whose likelihood wish evaluate. total evidence queries (.e., spanning variables conditioning events), faster also include arf function call. Note piecewise constant estimator considerably worse experiment. lik function can also used compute likelihood partial state, .e. setting variable values specified. Let’s take look iris dataset: Say want calculate sample likelihoods using continuous data. , provide values first four variables exclude fifth. case, model marginalize Species:  find likelihoods almost identical, slightly higher likelihood average partial samples. expected, since less variation model. example, used data throughout. may lead overfitting. sufficient data, preferable use training set adversarial_rf, validation set forde, test set lik. Alternatively, can set oob argument TRUE either latter two functions, case computations performed --bag (OOB) data. samples randomly excluded given tree due bootstrapping subroutine RF classifier. Note works dataset x passed forde lik one used train arf. Recall sample’s probability excluded single tree \\(\\exp(-1) \\approx 0.368\\). using oob = TRUE, sure include enough trees every observation likely OOB least times.","code":"# Compute likelihood under truncated normal and uniform distributions ll <- lik(params_iris, iris, arf = arf_iris) ll_unif <- lik(params_unif, iris, arf = arf_iris)  # Compare average negative log-likelihood (lower is better) -mean(ll) #> [1] 0.5255625 -mean(ll_unif) #> [1] -4.363783 head(iris) #>   Sepal.Length Sepal.Width Petal.Length Petal.Width Species #> 1          5.1         3.5          1.4         0.2  setosa #> 2          4.9         3.0          1.4         0.2  setosa #> 3          4.7         3.2          1.3         0.2  setosa #> 4          4.6         3.1          1.5         0.2  setosa #> 5          5.0         3.6          1.4         0.2  setosa #> 6          5.4         3.9          1.7         0.4  setosa # Compute likelihoods after marginalizing over Species iris_without_species <- iris[, -5] ll_cnt <- lik(params_iris, iris_without_species)  # Compare results tmp <- data.frame(Total = ll, Partial = ll_cnt) ggplot(tmp, aes(Total, Partial)) +    geom_point() +    geom_abline(slope = 1, intercept = 0, color = 'red')"},{"path":"https://bips-hb.github.io/arf/articles/arf.html","id":"data-synthesis","dir":"Articles","previous_headings":"","what":"Data Synthesis","title":"Adversarial Random Forests","text":"experiment, use smiley simulation mlbench package, allows easy visual assessment. draw training set \\(n = 1000\\) simulate \\(1000\\) synthetic datapoints. Resulting data plotted side side.  general shape clearly recognizable, even stray samples evident borders always crisp. can improved training data.","code":"# Simulate training data library(mlbench) x <- mlbench.smiley(1000) x <- data.frame(x$x, x$classes) colnames(x) <- c('X', 'Y', 'Class')  # Fit ARF arf_smiley <- adversarial_rf(x, mtry = 2) #> Iteration: 0, Accuracy: 90.68% #> Iteration: 1, Accuracy: 38.64% # Estimate parameters params_smiley <- forde(arf_smiley, x)  # Simulate data synth <- forge(params_smiley, n_synth = 1000)  # Compare structure str(x) #> 'data.frame':    1000 obs. of  3 variables: #>  $ X    : num  -0.841 -0.911 -0.91 -0.743 -0.863 ... #>  $ Y    : num  0.874 0.926 1.051 0.918 1.157 ... #>  $ Class: Factor w/ 4 levels \"1\",\"2\",\"3\",\"4\": 1 1 1 1 1 1 1 1 1 1 ... str(synth) #> 'data.frame':    1000 obs. of  3 variables: #>  $ X    : num  0.779 -0.721 0.645 0.643 0.97 ... #>  $ Y    : num  -0.446 0.937 -0.529 -0.569 0.983 ... #>  $ Class: Factor w/ 4 levels \"1\",\"3\",\"2\",\"4\": 4 1 4 4 3 4 1 2 2 4 ... # Put it all together x$Data <- 'Original' synth$Data <- 'Synthetic' df <- rbind(x, synth)  # Plot results ggplot(df, aes(X, Y, color = Class, shape = Class)) +    geom_point() +    facet_wrap(~ Data)"},{"path":"https://bips-hb.github.io/arf/articles/arf.html","id":"conditioning","dir":"Articles","previous_headings":"","what":"Conditioning","title":"Adversarial Random Forests","text":"ARFs can also used compute likelihoods synthesize data conditioning events specify values ranges input features. instance, say want evaluate likelihood samples iris dataset condition Species = 'setosa'. several ways encode evidence, simplest pass partial observation.  expected, measurements non-setosa samples appear much less likely conditioning event. partial observation method passing evidence requires users specify unique feature values conditioning variable. flexible alternative construct data frame conditioning events, potentially including inequalities. example, may want calculate likelihood samples given Species = 'setosa' Petal.Width > 0.3. can also define intervals: Resulting likelihoods computed condition queries drawn setosa flowers petal width interval \\((0.3, 0.5)\\). final method passing evidence directly compute posterior distribution leaves. useful particularly complex conditioning events currently inbuilt interface, polynomial constraints arbitrary propositions disjunctive normal form. case, just require data frame columns f_idx wt. latter sum unity, distribution normalized warning. methods can used conditional sampling well.  conditioning Class = 4, restrict sampling smile , rather eyes nose. Computing conditional expectations similarly straightforward. average \\(X, Y\\) coordinates smile .","code":"# Compute conditional likelihoods evi <- data.frame(Species = 'setosa') ll_conditional <- lik(params_iris, query = iris_without_species, evidence = evi)  # Compare NLL across species (shifting to positive range for visualization) tmp <- iris tmp$NLL <- -ll_conditional + max(ll_conditional) + 1 ggplot(tmp, aes(Species, NLL, fill = Species)) +    geom_boxplot() +    scale_y_log10() +    ylab('Negative Log-Likelihood') +    theme(legend.position = 'none') #> Warning: Removed 6 rows containing non-finite outside the scale range #> (`stat_boxplot()`). # Data frame of conditioning events evi <- data.frame(Species = \"setosa\",                    Petal.Width = \">0.3\") evi #>   Species Petal.Width #> 1  setosa        >0.3 evi <- data.frame(Species = \"setosa\",                    Petal.Width = \"(0.3,0.5)\") evi #>   Species Petal.Width #> 1  setosa   (0.3,0.5) # Drawing random weights evi <- data.frame(f_idx = params_iris$forest$f_idx,                   wt = rexp(nrow(params_iris$forest))) evi$wt <- evi$wt / sum(evi$wt) head(evi) #>   f_idx           wt #> 1     1 6.787756e-05 #> 2     2 2.290688e-04 #> 3     3 7.573631e-05 #> 4     4 2.252819e-05 #> 5     5 5.651368e-04 #> 6     6 5.034869e-04 # Simulate class-conditional data for smiley example evi <- data.frame(Class = 4) synth2 <- forge(params_smiley, n_synth = 250, evidence = evi)  # Put it all together synth2$Data <- 'Synthetic' df <- rbind(x, synth2)  # Plot results ggplot(df, aes(X, Y, color = Class, shape = Class)) +    geom_point() +    facet_wrap(~ Data) expct(params_smiley, evidence = evi) #>             X          Y #> 1 -0.01207312 -0.6730464"},{"path":"https://bips-hb.github.io/arf/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Marvin N. Wright. Author, maintainer. David S. Watson. Author. Kristin Blesch. Author. Jan Kapar. Author.","code":""},{"path":"https://bips-hb.github.io/arf/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Wright M, Watson D, Blesch K, Kapar J (2024). arf: Adversarial Random Forests. R package version 0.2.2,  https://bips-hb.github.io/arf/, https://github.com/bips-hb/arf.","code":"@Manual{,   title = {arf: Adversarial Random Forests},   author = {Marvin N. Wright and David S. Watson and Kristin Blesch and Jan Kapar},   year = {2024},   note = {R package version 0.2.2,  https://bips-hb.github.io/arf/},   url = {https://github.com/bips-hb/arf}, }"},{"path":[]},{"path":"https://bips-hb.github.io/arf/index.html","id":"introduction","dir":"","previous_headings":"","what":"Introduction","title":"Adversarial Random Forests","text":"Adversarial random forests (ARFs) recursively partition data fully factorized leaves, features jointly independent. procedure iterative, alternating rounds generation discrimination. Data become increasingly realistic round, original synthetic samples can longer reliably distinguished. useful several unsupervised learning tasks, density estimation data synthesis. Methods implemented package. ARFs naturally handle unstructured data mixed continuous categorical covariates. inherit many benefits RFs, including speed, flexibility, solid performance default parameters.","code":""},{"path":"https://bips-hb.github.io/arf/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Adversarial Random Forests","text":"arf package available CRAN: install development version GitHub using devtools, run:","code":"install.packages(\"arf\") devtools::install_github(\"bips-hb/arf\")"},{"path":"https://bips-hb.github.io/arf/index.html","id":"examples","dir":"","previous_headings":"","what":"Examples","title":"Adversarial Random Forests","text":"Using Fisher’s iris dataset, train ARF estimate distribution parameters:","code":"# Train the ARF arf <- adversarial_rf(iris)  # Estimate distribution parameters psi <- forde(arf, iris)"},{"path":"https://bips-hb.github.io/arf/index.html","id":"density-estimation","dir":"","previous_headings":"Examples","what":"Density estimation","title":"Adversarial Random Forests","text":"estimate log-likelihoods:","code":"mean(lik(arf, psi, iris))"},{"path":"https://bips-hb.github.io/arf/index.html","id":"generative-modeling","dir":"","previous_headings":"Examples","what":"Generative modeling","title":"Adversarial Random Forests","text":"generate 100 synthetic samples:","code":"forge(psi, 100)"},{"path":"https://bips-hb.github.io/arf/index.html","id":"conditional-expectations","dir":"","previous_headings":"Examples","what":"Conditional expectations","title":"Adversarial Random Forests","text":"estimate mean variable(s), optionally conditioned event(s): detailed examples, see package vignette.","code":"evi <- data.frame(Species = \"setosa\") expct(psi, query = \"Sepal.Length\", evidence = evi)"},{"path":"https://bips-hb.github.io/arf/index.html","id":"python-library","dir":"","previous_headings":"","what":"Python library","title":"Adversarial Random Forests","text":"Python implementation ARF, arfpy, available PyPI. development version, see .","code":""},{"path":"https://bips-hb.github.io/arf/index.html","id":"references","dir":"","previous_headings":"","what":"References","title":"Adversarial Random Forests","text":"Watson, D. S., Blesch, K., Kapar, J. & Wright, M. N. (2023). Adversarial random forests density estimation generative modeling. Proceedings 26th International Conference Artificial Intelligence Statistics. Link .","code":""},{"path":"https://bips-hb.github.io/arf/reference/adversarial_rf.html","id":null,"dir":"Reference","previous_headings":"","what":"Adversarial Random Forests — adversarial_rf","title":"Adversarial Random Forests — adversarial_rf","text":"Implements adversarial random forest learn independence-inducing splits.","code":""},{"path":"https://bips-hb.github.io/arf/reference/adversarial_rf.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Adversarial Random Forests — adversarial_rf","text":"","code":"adversarial_rf(   x,   num_trees = 10L,   min_node_size = 2L,   delta = 0,   max_iters = 10L,   early_stop = TRUE,   prune = TRUE,   verbose = TRUE,   parallel = TRUE,   ... )"},{"path":"https://bips-hb.github.io/arf/reference/adversarial_rf.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Adversarial Random Forests — adversarial_rf","text":"x Input data. Integer variables recoded ordered factors warning. See Details. num_trees Number trees grow forest. default works well generative modeling tasks, increased likelihood estimation. See Details. min_node_size Minimal number real data samples leaf nodes. delta Tolerance parameter. Algorithm converges OOB accuracy < 0.5 + delta. max_iters Maximum iterations adversarial loop. early_stop Terminate loop performance fails improve one round next? prune Impose min_node_size pruning? verbose Print discriminator accuracy round? parallel Compute parallel? Must register backend beforehand, e.g. via doParallel. ... Extra parameters passed ranger.","code":""},{"path":"https://bips-hb.github.io/arf/reference/adversarial_rf.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Adversarial Random Forests — adversarial_rf","text":"random forest object class ranger.","code":""},{"path":"https://bips-hb.github.io/arf/reference/adversarial_rf.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Adversarial Random Forests — adversarial_rf","text":"adversarial random forest (ARF) algorithm partitions data fully factorized leaves features jointly independent. ARFs trained iteratively, alternating rounds generation discrimination. first instance, synthetic data generated via independent bootstraps feature, RF classifier trained distinguish real fake samples. subsequent rounds, synthetic data generated separately leaf, using splits previous forest. creates increasingly realistic data satisfies local independence construction. algorithm converges RF reliably distinguish two classes, .e. OOB accuracy falls 0.5 + delta. ARFs useful several unsupervised learning tasks, density estimation (see forde) data synthesis (see forge). former, recommend increasing number trees improved performance (typically order 100-1000 depending sample size). Integer variables recoded warning. Default behavior convert six unique values numeric, five unique values treated ordered factors. override behavior, explicitly recode integer variables target type prior training. Note: convergence guaranteed finite samples. max_iters argument sets upper bound number training rounds. Similar results may attained increasing delta. Even single round can often give good performance, data strong complex dependencies may require iterations. default early_stop = TRUE, adversarial loop terminates performance improve one round next, case training may pointless.","code":""},{"path":"https://bips-hb.github.io/arf/reference/adversarial_rf.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Adversarial Random Forests — adversarial_rf","text":"Watson, D., Blesch, K., Kapar, J., & Wright, M. (2023). Adversarial random forests density estimation generative modeling. Proceedings 26th International Conference Artificial Intelligence Statistics, pp. 5357-5375.","code":""},{"path":[]},{"path":"https://bips-hb.github.io/arf/reference/adversarial_rf.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Adversarial Random Forests — adversarial_rf","text":"","code":"# Train ARF and estimate leaf parameters arf <- adversarial_rf(iris) #> Iteration: 0, Accuracy: 86.2% #> Iteration: 1, Accuracy: 40.2% #> Warning: executing %dopar% sequentially: no parallel backend registered psi <- forde(arf, iris)  # Generate 100 synthetic samples from the iris dataset x_synth <- forge(psi, n_synth = 100)  # Condition on Species = \"setosa\" and Sepal.Length > 6 evi <- data.frame(Species = \"setosa\",                   Sepal.Length = \"(6, Inf)\") x_synth <- forge(psi, n_synth = 100, evidence = evi)  # Estimate average log-likelihood ll <- lik(psi, iris, arf = arf, log = TRUE) mean(ll) #> [1] -0.6882583  # Expectation of Sepal.Length for class setosa evi <- data.frame(Species = \"setosa\") expct(psi, query = \"Sepal.Length\", evidence = evi) #>   Sepal.Length #> 1      5.01579"},{"path":"https://bips-hb.github.io/arf/reference/arf-package.html","id":null,"dir":"Reference","previous_headings":"","what":"arf: Adversarial Random Forests — arf-package","title":"arf: Adversarial Random Forests — arf-package","text":"Adversarial random forests (ARFs) recursively partition data fully factorized leaves, features jointly independent. procedure iterative, alternating rounds generation discrimination. Data becomes increasingly realistic round, original synthetic samples can longer reliably distinguished. useful several unsupervised learning tasks, density estimation data synthesis. Methods implemented package. ARFs naturally handle unstructured data mixed continuous categorical covariates. inherit many benefits random forests, including speed, flexibility, solid performance default parameters. details, see Watson et al. (2022) arXiv:2205.09435.","code":""},{"path":[]},{"path":"https://bips-hb.github.io/arf/reference/arf-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"arf: Adversarial Random Forests — arf-package","text":"Maintainer: Marvin N. Wright cran@wrig.de (ORCID) Authors: David S. Watson david.s.watson11@gmail.com (ORCID) Kristin Blesch (ORCID) Jan Kapar (ORCID)","code":""},{"path":"https://bips-hb.github.io/arf/reference/arf-package.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"arf: Adversarial Random Forests — arf-package","text":"","code":"# Train ARF and estimate leaf parameters arf <- adversarial_rf(iris) #> Iteration: 0, Accuracy: 84.75% #> Iteration: 1, Accuracy: 41.75% psi <- forde(arf, iris)  # Generate 100 synthetic samples from the iris dataset x_synth <- forge(psi, n_synth = 100)  # Condition on Species = \"setosa\" and Sepal.Length > 6 evi <- data.frame(Species = \"setosa\",                   Sepal.Length = \"(6, Inf)\") x_synth <- forge(psi, n_synth = 100, evidence = evi)  # Estimate average log-likelihood ll <- lik(psi, iris, arf = arf, log = TRUE) mean(ll) #> [1] -0.7161056  # Expectation of Sepal.Length for class setosa evi <- data.frame(Species = \"setosa\") expct(psi, query = \"Sepal.Length\", evidence = evi) #>   Sepal.Length #> 1     5.012414"},{"path":"https://bips-hb.github.io/arf/reference/cforde.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute conditional circuit parameters — cforde","title":"Compute conditional circuit parameters — cforde","text":"Compute conditional circuit parameters","code":""},{"path":"https://bips-hb.github.io/arf/reference/cforde.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute conditional circuit parameters — cforde","text":"","code":"cforde(   params,   evidence,   row_mode = c(\"separate\", \"or\"),   stepsize = 0,   parallel = TRUE )"},{"path":"https://bips-hb.github.io/arf/reference/cforde.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute conditional circuit parameters — cforde","text":"params Circuit parameters learned via forde. evidence Data frame conditioning event(s). row_mode Interpretation rows multi-row conditions. stepsize Stepsize defining number condition rows handled one step. parallel Compute parallel? Must register backend beforehand, e.g. via doParallel.","code":""},{"path":"https://bips-hb.github.io/arf/reference/cforde.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute conditional circuit parameters — cforde","text":"List conditions (evidence_input), prepared conditions (evidence_prepped) leaves match conditions evidence continuous data (cnt) categorical data (cat) well leaf info (forest).","code":""},{"path":"https://bips-hb.github.io/arf/reference/col_rename.html","id":null,"dir":"Reference","previous_headings":"","what":"Adaptive column renaming — col_rename","title":"Adaptive column renaming — col_rename","text":"function renames columns case input colnames includes colnames required internal functions (e.g., \"y\").","code":""},{"path":"https://bips-hb.github.io/arf/reference/col_rename.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Adaptive column renaming — col_rename","text":"","code":"col_rename(cn, old_name)"},{"path":"https://bips-hb.github.io/arf/reference/col_rename.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Adaptive column renaming — col_rename","text":"cn Column names. old_name Name column renamed.","code":""},{"path":"https://bips-hb.github.io/arf/reference/col_rename_all.html","id":null,"dir":"Reference","previous_headings":"","what":"Rename all problematic columns with col_rename(). — col_rename_all","title":"Rename all problematic columns with col_rename(). — col_rename_all","text":"Rename problematic columns col_rename().","code":""},{"path":"https://bips-hb.github.io/arf/reference/col_rename_all.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Rename all problematic columns with col_rename(). — col_rename_all","text":"","code":"col_rename_all(cn)"},{"path":"https://bips-hb.github.io/arf/reference/col_rename_all.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Rename all problematic columns with col_rename(). — col_rename_all","text":"cn Old column names.","code":""},{"path":"https://bips-hb.github.io/arf/reference/col_rename_all.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Rename all problematic columns with col_rename(). — col_rename_all","text":"New columns names.","code":""},{"path":"https://bips-hb.github.io/arf/reference/expct.html","id":null,"dir":"Reference","previous_headings":"","what":"Expected Value — expct","title":"Expected Value — expct","text":"Compute expectation query variable(s), optionally conditioned event(s).","code":""},{"path":"https://bips-hb.github.io/arf/reference/expct.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Expected Value — expct","text":"","code":"expct(   params,   query = NULL,   evidence = NULL,   evidence_row_mode = c(\"separate\", \"or\"),   round = FALSE,   stepsize = 0,   parallel = TRUE )"},{"path":"https://bips-hb.github.io/arf/reference/expct.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Expected Value — expct","text":"params Circuit parameters learned via forde. query Optional character vector variable names. Estimates computed . NULL, variables evidence estimated. evidence contains NAs, variables estimated full dataset returned. evidence Optional set conditioning events. can take one three forms: (1) partial sample, .e. single row data columns; (2) data frame conditioning events, allows inequalities intervals; (3) posterior distribution leaves; see Details Examples. evidence_row_mode Interpretation rows multi-row evidence. 'separate', row evidence separate conditioning event n_synth synthetic samples generated. '', rows combined logical ; see Examples. round Round continuous variables respective maximum precision real data set? stepsize Stepsize defining number evidence rows handled one step. Defaults nrow(evidence)/num_registered_workers parallel == TRUE. parallel Compute parallel? Must register backend beforehand, e.g. via doParallel.","code":""},{"path":"https://bips-hb.github.io/arf/reference/expct.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Expected Value — expct","text":"one row data frame values query variables.","code":""},{"path":"https://bips-hb.github.io/arf/reference/expct.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Expected Value — expct","text":"function computes expected values subset features, optionally conditioned event(s). three methods (optionally) encoding conditioning events via evidence argument. first provide partial sample, columns training data missing set NA. second provide data frame condition events. supports inequalities intervals. Alternatively, users may directly input pre-calculated posterior distribution leaves, columns f_idx wt. may preferable complex constraints. See Examples.","code":""},{"path":"https://bips-hb.github.io/arf/reference/expct.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Expected Value — expct","text":"Watson, D., Blesch, K., Kapar, J., & Wright, M. (2023). Adversarial random forests density estimation generative modeling. Proceedings 26th International Conference Artificial Intelligence Statistics, pp. 5357-5375.","code":""},{"path":[]},{"path":"https://bips-hb.github.io/arf/reference/expct.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Expected Value — expct","text":"","code":"# Train ARF and estimate leaf parameters arf <- adversarial_rf(iris) #> Iteration: 0, Accuracy: 86.2% #> Iteration: 1, Accuracy: 44.63% psi <- forde(arf, iris)  # What is the expected value of Sepal.Length? expct(psi, query = \"Sepal.Length\") #>   Sepal.Length #> 1     5.843333  # What if we condition on Species = \"setosa\"? evi <- data.frame(Species = \"setosa\") expct(psi, query = \"Sepal.Length\", evidence = evi) #>   Sepal.Length #> 1     5.011448  # Compute expectations for all features other than Species expct(psi, evidence = evi) #>   Sepal.Length Sepal.Width Petal.Length Petal.Width #> 1     5.011448    3.408849      1.50539   0.2649252  # Condition on first two data rows with some missing values evi <- iris[1:2,] evi[1, 1] <- NA_real_ evi[1, 5] <- NA_character_ evi[2, 2] <- NA_real_ x_synth <- expct(psi, evidence = evi)"},{"path":"https://bips-hb.github.io/arf/reference/forde.html","id":null,"dir":"Reference","previous_headings":"","what":"Forests for Density Estimation — forde","title":"Forests for Density Estimation — forde","text":"Uses pre-trained ARF model estimate leaf distribution parameters.","code":""},{"path":"https://bips-hb.github.io/arf/reference/forde.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Forests for Density Estimation — forde","text":"","code":"forde(   arf,   x,   oob = FALSE,   family = \"truncnorm\",   finite_bounds = c(\"no\", \"local\", \"global\"),   alpha = 0,   epsilon = 0,   parallel = TRUE )"},{"path":"https://bips-hb.github.io/arf/reference/forde.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Forests for Density Estimation — forde","text":"arf Pre-trained adversarial_rf. Alternatively, object class ranger. x Training data estimating parameters. oob use --bag samples parameter estimation? TRUE, x must dataset used train arf. family Distribution use density estimation continuous features. Current options include truncated normal (default family = \"truncnorm\") uniform (family = \"unif\"). See Details. finite_bounds Impose finite bounds continuous variables? 'local', infinite bounds shrinked empirical extrema within leaves. 'global', infinite bounds shrinked global empirical extrema. '' (default), impose finite bounds. alpha Optional pseudocount Laplace smoothing categorical features. avoids zero-mass points test data fall outside support training data. Effectively parametrizes flat Dirichlet prior multinomial likelihoods. epsilon Optional slack parameter empirical bounds finite_bounds != ''. avoids zero-density points test data fall outside support training data. gap lower upper bounds expanded factor 1 + epsilon. parallel Compute parallel? Must register backend beforehand, e.g. via doParallel.","code":""},{"path":"https://bips-hb.github.io/arf/reference/forde.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Forests for Density Estimation — forde","text":"list 5 elements: (1) parameters continuous data; (2) parameters discrete data; (3) leaf indices coverage; (4) metadata variables; (5) data input class. list used estimating likelihoods lik generating data forge.","code":""},{"path":"https://bips-hb.github.io/arf/reference/forde.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Forests for Density Estimation — forde","text":"forde extracts leaf parameters pretrained forest learns distribution parameters data within leaf. former includes coverage (proportion data falling leaf) split criteria. latter includes proportions categorical features mean/variance continuous features. result probabilistic circuit, stored data.table, can used various downstream inference tasks. Currently, forde provides support limited number distributional families: truncated normal uniform continuous data, multinomial discrete data. Future releases accommodate larger set options. Though forde designed take adversarial random forest input, function's first argument can principle object class ranger. allows users test performance alternative pipelines (e.g., supervised forest input). also requirement x data used fit arf, unless oob = TRUE. fact, using another dataset may protect overfitting. connects Wager & Athey's (2018) notion \"honest trees\".","code":""},{"path":"https://bips-hb.github.io/arf/reference/forde.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Forests for Density Estimation — forde","text":"Watson, D., Blesch, K., Kapar, J., & Wright, M. (2023). Adversarial random forests density estimation generative modeling. Proceedings 26th International Conference Artificial Intelligence Statistics, pp. 5357-5375. Wager, S. & Athey, S. (2018). Estimation inference heterogeneous treatment effects using random forests. J. . Stat. Assoc., 113(523): 1228-1242.","code":""},{"path":[]},{"path":"https://bips-hb.github.io/arf/reference/forde.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Forests for Density Estimation — forde","text":"","code":"# Train ARF and estimate leaf parameters arf <- adversarial_rf(iris) #> Iteration: 0, Accuracy: 89.53% #> Iteration: 1, Accuracy: 43.62% psi <- forde(arf, iris)  # Generate 100 synthetic samples from the iris dataset x_synth <- forge(psi, n_synth = 100)  # Condition on Species = \"setosa\" and Sepal.Length > 6 evi <- data.frame(Species = \"setosa\",                   Sepal.Length = \"(6, Inf)\") x_synth <- forge(psi, n_synth = 100, evidence = evi)  # Estimate average log-likelihood ll <- lik(psi, iris, arf = arf, log = TRUE) mean(ll) #> [1] -0.8345415  # Expectation of Sepal.Length for class setosa evi <- data.frame(Species = \"setosa\") expct(psi, query = \"Sepal.Length\", evidence = evi) #>   Sepal.Length #> 1     5.014798"},{"path":"https://bips-hb.github.io/arf/reference/forge.html","id":null,"dir":"Reference","previous_headings":"","what":"Forests for Generative Modeling — forge","title":"Forests for Generative Modeling — forge","text":"Uses pre-trained FORDE model simulate synthetic data.","code":""},{"path":"https://bips-hb.github.io/arf/reference/forge.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Forests for Generative Modeling — forge","text":"","code":"forge(   params,   n_synth,   evidence = NULL,   evidence_row_mode = c(\"separate\", \"or\"),   round = TRUE,   sample_NAs = FALSE,   stepsize = 0,   parallel = TRUE )"},{"path":"https://bips-hb.github.io/arf/reference/forge.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Forests for Generative Modeling — forge","text":"params Circuit parameters learned via forde. n_synth Number synthetic samples generate. evidence Optional set conditioning events. can take one three forms: (1) partial sample, .e. single row data columns; (2) data frame conditioning events, allows inequalities intervals; (3) posterior distribution leaves; see Details Examples. evidence_row_mode Interpretation rows multi-row evidence. 'separate', row evidence separate conditioning event n_synth synthetic samples generated. '', rows combined logical ; see Examples. round Round continuous variables respective maximum precision real data set? sample_NAs Sample NAs respecting probability missing values original data. stepsize Stepsize defining number evidence rows handled one step. Defaults nrow(evidence)/num_registered_workers parallel == TRUE. parallel Compute parallel? Must register backend beforehand, e.g. via doParallel.","code":""},{"path":"https://bips-hb.github.io/arf/reference/forge.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Forests for Generative Modeling — forge","text":"dataset n_synth synthetic samples.","code":""},{"path":"https://bips-hb.github.io/arf/reference/forge.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Forests for Generative Modeling — forge","text":"forge simulates synthetic dataset n_synth samples. First, leaves sampled proportion either coverage (evidence = NULL) posterior probability. , feature sampled independently within leaf according probability mass density function learned forde. create realistic data long adversarial RF used previous step satisfies local independence criterion. See Watson et al. (2023). three methods (optionally) encoding conditioning events via evidence argument. first provide partial sample, columns training data missing set NA. second provide data frame condition events. supports inequalities intervals. Alternatively, users may directly input pre-calculated posterior distribution leaves, columns f_idx wt. may preferable complex constraints. See Examples.","code":""},{"path":"https://bips-hb.github.io/arf/reference/forge.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Forests for Generative Modeling — forge","text":"Watson, D., Blesch, K., Kapar, J., & Wright, M. (2023). Adversarial random forests density estimation generative modeling. Proceedings 26th International Conference Artificial Intelligence Statistics, pp. 5357-5375.","code":""},{"path":[]},{"path":"https://bips-hb.github.io/arf/reference/forge.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Forests for Generative Modeling — forge","text":"","code":"# Train ARF and estimate leaf parameters arf <- adversarial_rf(iris) #> Iteration: 0, Accuracy: 90.3% #> Iteration: 1, Accuracy: 37.25% psi <- forde(arf, iris)  # Generate 100 synthetic samples from the iris dataset x_synth <- forge(psi, n_synth = 100)  # Condition on Species = \"setosa\" evi <- data.frame(Species = \"setosa\") x_synth <- forge(psi, n_synth = 100, evidence = evi)  # Condition on Species = \"setosa\" and Sepal.Length > 6 evi <- data.frame(Species = \"setosa\",                   Sepal.Length = \"(6, Inf)\") x_synth <- forge(psi, n_synth = 100, evidence = evi)  # Alternative syntax for <\/> conditions evi <- data.frame(Sepal.Length = \">6\") x_synth <- forge(psi, n_synth = 100, evidence = evi)  # Negation condition, i.e. all classes except \"setosa\" evi <- data.frame(Species = \"!setosa\") x_synth <- forge(psi, n_synth = 100, evidence = evi)  # Condition on first two data rows with some missing values evi <- iris[1:2,] evi[1, 1] <- NA_real_ evi[1, 5] <- NA_character_ evi[2, 2] <- NA_real_ x_synth <- forge(psi, n_synth = 1, evidence = evi)  # Or just input some distribution on leaves # (Weights that do not sum to unity are automatically scaled) n_leaves <- nrow(psi$forest) evi <- data.frame(f_idx = psi$forest$f_idx, wt = rexp(n_leaves)) x_synth <- forge(psi, n_synth = 100, evidence = evi)"},{"path":"https://bips-hb.github.io/arf/reference/lik.html","id":null,"dir":"Reference","previous_headings":"","what":"Likelihood Estimation — lik","title":"Likelihood Estimation — lik","text":"Compute likelihood input data, optionally conditioned event(s).","code":""},{"path":"https://bips-hb.github.io/arf/reference/lik.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Likelihood Estimation — lik","text":"","code":"lik(   params,   query,   evidence = NULL,   arf = NULL,   oob = FALSE,   log = TRUE,   batch = NULL,   parallel = TRUE )"},{"path":"https://bips-hb.github.io/arf/reference/lik.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Likelihood Estimation — lik","text":"params Circuit parameters learned via forde. query Data frame samples, optionally comprising just subset training features. Likelihoods computed sample. Missing features marginalized . See Details. evidence Optional set conditioning events. can take one three forms: (1) partial sample, .e. single row data columns; (2) data frame conditioning events, allows inequalities; (3) posterior distribution leaves. See Details. arf Pre-trained adversarial_rf object class ranger. required speeds computation considerably total evidence queries. (Ignored partial evidence queries.) oob use --bag leaves likelihood estimation? TRUE, x must dataset used train arf. applicable total evidence queries. log Return likelihoods log scale? Recommended prevent underflow. batch Batch size. default compute densities queries one round, always fastest option memory allows. However, large samples many trees, can memory efficient split data batches. impact results. parallel Compute parallel? Must register backend beforehand, e.g. via doParallel.","code":""},{"path":"https://bips-hb.github.io/arf/reference/lik.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Likelihood Estimation — lik","text":"vector likelihoods, optionally log scale.","code":""},{"path":"https://bips-hb.github.io/arf/reference/lik.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Likelihood Estimation — lik","text":"function computes likelihood input data, optionally conditioned event(s). Queries may partial, .e. covering features, case excluded variables marginalized . three methods (optionally) encoding conditioning events via evidence argument. first provide partial sample, columns training data present. second provide data frame three columns: variable, relation, value. supports inequalities via relation. Alternatively, users may directly input pre-calculated posterior distribution leaves, columns f_idx wt. may preferable complex constraints. See Examples.","code":""},{"path":"https://bips-hb.github.io/arf/reference/lik.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Likelihood Estimation — lik","text":"Watson, D., Blesch, K., Kapar, J., & Wright, M. (2023). Adversarial random forests density estimation generative modeling. Proceedings 26th International Conference Artificial Intelligence Statistics, pp. 5357-5375.","code":""},{"path":[]},{"path":"https://bips-hb.github.io/arf/reference/lik.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Likelihood Estimation — lik","text":"","code":"# Train ARF and estimate leaf parameters arf <- adversarial_rf(iris) #> Iteration: 0, Accuracy: 83.28% #> Iteration: 1, Accuracy: 33.67% psi <- forde(arf, iris)  # Estimate average log-likelihood ll <- lik(psi, iris, arf = arf, log = TRUE) mean(ll) #> [1] -0.6067289  # Identical but slower ll <- lik(psi, iris, log = TRUE) #> Warning: For total evidence queries, it is faster to include the pre-trained arf. mean(ll) #> [1] -0.6067289  # Partial evidence query lik(psi, query = iris[1, 1:3]) #> [1] 0.5948271  # Condition on Species = \"setosa\" evi <- data.frame(Species = \"setosa\") lik(psi, query = iris[1, 1:3], evidence = evi) #> [1] 1.693439  # Condition on Species = \"setosa\" and Petal.Width > 0.3 evi <- data.frame(Species = \"setosa\",                    Petal.Width = \">0.3\") lik(psi, query = iris[1, 1:3], evidence = evi) #> [1] 1.048104"},{"path":"https://bips-hb.github.io/arf/reference/post_x.html","id":null,"dir":"Reference","previous_headings":"","what":"Post-process data — post_x","title":"Post-process data — post_x","text":"function prepares output data forge.","code":""},{"path":"https://bips-hb.github.io/arf/reference/post_x.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Post-process data — post_x","text":"","code":"post_x(x, params, round = TRUE)"},{"path":"https://bips-hb.github.io/arf/reference/post_x.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Post-process data — post_x","text":"x Input data.frame. params Circuit parameters learned via forde. round Round continuous variables respective maximum precision real data set?","code":""},{"path":"https://bips-hb.github.io/arf/reference/prep_cond.html","id":null,"dir":"Reference","previous_headings":"","what":"Preprocess conditions — prep_cond","title":"Preprocess conditions — prep_cond","text":"function prepares conditions computing conditional circuit paramaters via cforde","code":""},{"path":"https://bips-hb.github.io/arf/reference/prep_cond.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Preprocess conditions — prep_cond","text":"","code":"prep_cond(evidence, params, row_mode)"},{"path":"https://bips-hb.github.io/arf/reference/prep_cond.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Preprocess conditions — prep_cond","text":"evidence Optional set conditioning events. params Circuit parameters learned via forde. row_mode Interpretation rows multi-row conditions.","code":""},{"path":"https://bips-hb.github.io/arf/reference/prep_x.html","id":null,"dir":"Reference","previous_headings":"","what":"Preprocess input data — prep_x","title":"Preprocess input data — prep_x","text":"function prepares input data ARFs.","code":""},{"path":"https://bips-hb.github.io/arf/reference/prep_x.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Preprocess input data — prep_x","text":"","code":"prep_x(x)"},{"path":"https://bips-hb.github.io/arf/reference/prep_x.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Preprocess input data — prep_x","text":"x Input data.frame.","code":""},{"path":"https://bips-hb.github.io/arf/reference/resample.html","id":null,"dir":"Reference","previous_headings":"","what":"Safer version of sample() — resample","title":"Safer version of sample() — resample","text":"Safer version sample()","code":""},{"path":"https://bips-hb.github.io/arf/reference/resample.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Safer version of sample() — resample","text":"","code":"resample(x, ...)"},{"path":"https://bips-hb.github.io/arf/reference/resample.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Safer version of sample() — resample","text":"x vector one elements choose. ... arguments sample().","code":""},{"path":"https://bips-hb.github.io/arf/reference/resample.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Safer version of sample() — resample","text":"vector length size elements drawn x.","code":""},{"path":"https://bips-hb.github.io/arf/reference/which.max.random.html","id":null,"dir":"Reference","previous_headings":"","what":"which.max() with random at ties — which.max.random","title":"which.max() with random at ties — which.max.random","text":".max() random ties","code":""},{"path":"https://bips-hb.github.io/arf/reference/which.max.random.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"which.max() with random at ties — which.max.random","text":"","code":"which.max.random(x)"},{"path":"https://bips-hb.github.io/arf/reference/which.max.random.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"which.max() with random at ties — which.max.random","text":"x numeric vector.","code":""},{"path":"https://bips-hb.github.io/arf/reference/which.max.random.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"which.max() with random at ties — which.max.random","text":"Index maximum value x, random tie-breaking.","code":""},{"path":"https://bips-hb.github.io/arf/news/index.html","id":"arf-022","dir":"Changelog","previous_headings":"","what":"arf 0.2.2","title":"arf 0.2.2","text":"Faster vectorized conditional sampling Use min.bucket argument ranger avoid pruning possible Option sample NAs generated data original data contains NAs Stepsize forge() reduce memory usage Option local global finite bounds","code":""},{"path":"https://bips-hb.github.io/arf/news/index.html","id":"arf-020","dir":"Changelog","previous_headings":"","what":"arf 0.2.0","title":"arf 0.2.0","text":"CRAN release: 2024-01-24 Vectorized adversarial resampling Speed boost compiling probabilistic circuit Conditional densities sampling Bayesian solution invariant continuous data within leaf nodes New function computing (conditional) expectations Options missing data","code":""},{"path":"https://bips-hb.github.io/arf/news/index.html","id":"arf-013","dir":"Changelog","previous_headings":"","what":"arf 0.1.3","title":"arf 0.1.3","text":"CRAN release: 2023-02-06 Speed boost adversarial resampling step Early stopping option adversarial training alpha parameter regularizing multinomial distributions forde Unified treatment colnames internal semantics (y, obs, tree, leaf)","code":""}]
