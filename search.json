[{"path":"https://bips-hb.github.io/arf/articles/arf.html","id":"adversarial-training","dir":"Articles","previous_headings":"","what":"Adversarial Training","title":"Adversarial Random Forests","text":"ARF algorithm iterative procedure. first instance, generate synthetic data independently sampling marginals feature training random forest (RF) distinguish original synthetic samples. accuracy greater \\(0.5 + \\delta\\) (delta user-controlled tolerance parameter, generally set 0), create new dataset sampling marginals within leaf training another RF classifier. procedure repeats original synthetic samples reliably distinguished. default verbose = TRUE, algorithm print accuracy iteration. printouts can turned setting verbose = FALSE. Accuracy still stored within arf object, can evaluate convergence fact. warning appears just per session. can suppressed setting parallel = FALSE registering parallel backend ().  find quick drop accuracy following resampling procedure, desired. ARF converged, resulting splits form fully factorized leaves, .e. subregions feature space variables locally independent. ARF convergence asymptotically guaranteed \\(n \\rightarrow \\infty\\) (see Watson et al., 2023, Thm. 1). However, implications finite sample performance. practice, often find adversarial training completes just one two rounds, may hold datasets. avoid infinite loops, users can increase slack parameter delta set max_iters argument (default = 10). addition failsafes, adversarial_rf uses early stopping default (early_stop = TRUE), terminates training factorization improve one round next. recommended, since discriminator accuracy rarely falls much lower increased. density estimation tasks, recommend increasing default number trees. generally use 100 experiments, though may suboptimal datasets. Likelihood estimates sensitive parameter certain threshold, larger models incur extra costs time memory. can speed computations registering parallel backend, case ARF training distributed across cores using ranger package. Much like ranger, default behavior adversarial_rf compute parallel possible. exactly done varies across operating systems. following code works Unix machines. Windows requires different setup. either case, can now execute parallel. result object class ranger, can input downstream functions.","code":"# Load libraries library(arf) library(data.table) library(ggplot2)  # Set seed set.seed(123, \"L'Ecuyer-CMRG\")  # Train ARF arf_iris <- adversarial_rf(iris) #> Iteration: 0, Accuracy: 86.53% #> Iteration: 1, Accuracy: 40% #> Warning: executing %dopar% sequentially: no parallel backend registered # Train ARF with no printouts arf_iris <- adversarial_rf(iris, verbose = FALSE)  # Plot accuracy against iterations (model converges when accuracy <= 0.5) tmp <- data.frame('Accuracy' = arf_iris$acc,                    'Iteration' = seq_len(length(arf_iris$acc))) ggplot(tmp, aes(Iteration, Accuracy)) +    geom_point() +    geom_path() +   geom_hline(yintercept = 0.5, linetype = 'dashed', color = 'red') # Register cores - Unix library(doParallel) registerDoParallel(cores = 2) # Register cores - Windows library(doParallel) cl <- makeCluster(2) registerDoParallel(cl) # Rerun ARF, now in parallel and with more trees arf_iris <- adversarial_rf(iris, num_trees = 100) #> Iteration: 0, Accuracy: 92.33% #> Iteration: 1, Accuracy: 32%"},{"path":"https://bips-hb.github.io/arf/articles/arf.html","id":"parameter-learning","dir":"Articles","previous_headings":"","what":"Parameter Learning","title":"Adversarial Random Forests","text":"next step learn leaf distribution parameters using forests density estimation (FORDE). function calculates coverage, bounds, pdf/pmf parameters every variable every leaf. can expensive computation large datasets, requires \\(\\mathcal{O}\\big(B \\cdot d \\cdot n \\cdot \\log(n)\\big)\\) operations, \\(B\\) number trees, \\(d\\) data dimensionality, \\(n\\) sample size. , process parallelized default. Default behavior use truncated normal distribution continuous data (boundaries given tree’s split parameters) multinomial distribution categorical data. find produces stable results wide range settings. can also use uniform distribution continuous features setting family = 'unif', thereby instantiating piecewise constant density estimator. method tends perform poorly practice, recommend . option implemented primarily benchmarking purposes. Alternative families, e.g. truncated Poisson beta distributions, may useful certain problems. Future releases expand range options family argument. alpha epsilon arguments allow optional regularization multinomial uniform distributions, respectively. help prevent zero likelihood samples test data fall outside support training data. former pseudocount parameter applies Laplace smoothing within leaves, preventing unobserved values assigned zero probability unless splits explicitly rule . words, impose flat Dirichlet prior report posterior probabilities rather maximum likelihood estimates. latter slack parameter empirical bounds expands estimated extrema continuous features factor \\(1 + \\epsilon\\). Compare results original probability estimates Species variable obtained adding pseudocount \\(\\alpha = 0.1\\). Laplace smoothing, extreme probabilities occur splits explicitly demand . Otherwise, values shrink toward uniform prior. Note two data tables may exactly rows, omit zero probability events conserve memory. However, can verify probabilities sum unity leaf-variable combination. forde function outputs list length 6, entries (1) continuous features; (2) categorical features; (3) leaf parameters; (4) variable metadata; (5) factor levels; (6) data input class. parameters can used variety downstream tasks, likelihood estimation data synthesis.","code":"# Compute leaf and distribution parameters params_iris <- forde(arf_iris, iris) # Recompute with uniform density params_unif <- forde(arf_iris, iris, family = 'unif') # Recompute with additive smoothing params_alpha <- forde(arf_iris, iris, alpha = 0.1)  # Compare results head(params_iris$cat) #> Key: <f_idx, variable> #>    f_idx variable       val  prob #>    <int>   <char>    <char> <num> #> 1:     1  Species virginica     1 #> 2:     2  Species virginica     1 #> 3:     3  Species virginica     1 #> 4:     4  Species virginica     1 #> 5:     5  Species virginica     1 #> 6:     6  Species    setosa     1 head(params_alpha$cat) #> Key: <f_idx, variable> #>    f_idx variable        val       prob #>    <int>   <char>     <char>      <num> #> 1:     1  Species  virginica 0.93939394 #> 2:     1  Species     setosa 0.03030303 #> 3:     1  Species versicolor 0.03030303 #> 4:     2  Species  virginica 0.96825397 #> 5:     2  Species     setosa 0.01587302 #> 6:     2  Species versicolor 0.01587302 # Sum probabilities summary(params_iris$cat[, sum(prob), by = .(f_idx, variable)]$V1) #>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.  #>       1       1       1       1       1       1 summary(params_alpha$cat[, sum(prob), by = .(f_idx, variable)]$V1) #>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.  #>       1       1       1       1       1       1 params_iris #> $cnt #> Key: <f_idx, variable> #>       f_idx     variable   min   max        mu      sigma #>       <int>       <char> <num> <num>     <num>      <num> #>    1:     1 Petal.Length  -Inf   Inf 5.9333333 0.20816660 #>    2:     1  Petal.Width  2.45   Inf 2.5000000 0.01041469 #>    3:     1 Sepal.Length  -Inf   Inf 6.7333333 0.45092498 #>    4:     1  Sepal.Width  -Inf   Inf 3.4000000 0.17320508 #>    5:     2 Petal.Length  5.65   Inf 6.1166667 0.34302575 #>   ---                                                     #> 7144:  1786  Sepal.Width  -Inf  2.85 2.4666667 0.05773503 #> 7145:  1787 Petal.Length  -Inf  1.45 1.3833333 0.04082483 #> 7146:  1787  Petal.Width  -Inf  0.35 0.2166667 0.07527727 #> 7147:  1787 Sepal.Length  4.45  4.95 4.7333333 0.12110601 #> 7148:  1787  Sepal.Width  2.85  3.55 3.1333333 0.16329932 #>  #> $cat #> Key: <f_idx, variable> #>       f_idx variable        val  prob #>       <int>   <char>     <char> <num> #>    1:     1  Species  virginica 1.000 #>    2:     2  Species  virginica 1.000 #>    3:     3  Species  virginica 1.000 #>    4:     4  Species  virginica 1.000 #>    5:     5  Species  virginica 1.000 #>   ---                                 #> 2056:  1784  Species versicolor 1.000 #> 2057:  1785  Species     setosa 0.125 #> 2058:  1785  Species versicolor 0.875 #> 2059:  1786  Species versicolor 1.000 #> 2060:  1787  Species     setosa 1.000 #>  #> $forest #> Key: <tree, leaf> #>       f_idx  tree  leaf        cvg #>       <int> <int> <int>      <num> #>    1:     1     1     7 0.02000000 #>    2:     2     1    21 0.04000000 #>    3:     3     1    27 0.02000000 #>    4:     4     1    35 0.07333333 #>    5:     5     1    37 0.01333333 #>   ---                              #> 1783:  1783   100    83 0.04666667 #> 1784:  1784   100    85 0.04000000 #> 1785:  1785   100    86 0.05333333 #> 1786:  1786   100    87 0.02000000 #> 1787:  1787   100    88 0.04000000 #>  #> $meta #>        variable   class    family decimals #>          <char>  <char>    <char>    <int> #> 1: Sepal.Length numeric truncnorm        1 #> 2:  Sepal.Width numeric truncnorm        1 #> 3: Petal.Length numeric truncnorm        1 #> 4:  Petal.Width numeric truncnorm        1 #> 5:      Species  factor  multinom       NA #>  #> $levels #>    variable        val #>      <fctr>     <char> #> 1:  Species  virginica #> 2:  Species     setosa #> 3:  Species versicolor #>  #> $input_class #> [1] \"data.frame\""},{"path":"https://bips-hb.github.io/arf/articles/arf.html","id":"likelihood-estimation","dir":"Articles","previous_headings":"","what":"Likelihood Estimation","title":"Adversarial Random Forests","text":"calculate log-likelihoods, pass params lik function, along data whose likelihood wish evaluate. total evidence queries (.e., spanning variables conditioning events), faster also include arf function call. Note piecewise constant estimator considerably worse experiment. lik function can also used compute likelihood partial state, .e. setting variable values specified. Let’s take look iris dataset: Say want calculate sample likelihoods using continuous data. , provide values first four variables exclude fifth. case, model marginalize Species:  find likelihoods almost identical, slightly higher likelihood average partial samples. expected, since less variation model. example, used data throughout. may lead overfitting. sufficient data, preferable use training set adversarial_rf, validation set forde, test set lik. Alternatively, can set oob argument TRUE either latter two functions, case computations performed --bag (OOB) data. samples randomly excluded given tree due bootstrapping subroutine RF classifier. Note works dataset x passed forde lik one used train arf. Recall sample’s probability excluded single tree \\(\\exp(-1) \\approx 0.368\\). using oob = TRUE, sure include enough trees every observation likely OOB least times.","code":"# Compute likelihood under truncated normal and uniform distributions ll <- lik(params_iris, iris, arf = arf_iris) ll_unif <- lik(params_unif, iris, arf = arf_iris)  # Compare average negative log-likelihood (lower is better) -mean(ll) #> [1] 0.6762925 -mean(ll_unif) #> [1] 3.861917 head(iris) #>   Sepal.Length Sepal.Width Petal.Length Petal.Width Species #> 1          5.1         3.5          1.4         0.2  setosa #> 2          4.9         3.0          1.4         0.2  setosa #> 3          4.7         3.2          1.3         0.2  setosa #> 4          4.6         3.1          1.5         0.2  setosa #> 5          5.0         3.6          1.4         0.2  setosa #> 6          5.4         3.9          1.7         0.4  setosa # Compute likelihoods after marginalizing over Species iris_without_species <- iris[, -5] ll_cnt <- lik(params_iris, iris_without_species)  # Compare results tmp <- data.frame(Total = ll, Partial = ll_cnt) ggplot(tmp, aes(Total, Partial)) +    geom_point() +    geom_abline(slope = 1, intercept = 0, color = 'red')"},{"path":"https://bips-hb.github.io/arf/articles/arf.html","id":"data-synthesis","dir":"Articles","previous_headings":"","what":"Data Synthesis","title":"Adversarial Random Forests","text":"experiment, use smiley simulation mlbench package, allows easy visual assessment. draw training set \\(n = 1000\\) simulate \\(1000\\) synthetic datapoints. Resulting data plotted side side.  general shape clearly recognizable, even stray samples evident borders always crisp. can improved training data.","code":"# Simulate training data library(mlbench) x <- mlbench.smiley(1000) x <- data.frame(x$x, x$classes) colnames(x) <- c('X', 'Y', 'Class')  # Fit ARF arf_smiley <- adversarial_rf(x, mtry = 2) #> Iteration: 0, Accuracy: 90.17% #> Iteration: 1, Accuracy: 37.84%  # Estimate parameters params_smiley <- forde(arf_smiley, x)  # Simulate data synth <- forge(params_smiley, n_synth = 1000)  # Compare structure str(x) #> 'data.frame':    1000 obs. of  3 variables: #>  $ X    : num  -0.841 -0.911 -0.91 -0.743 -0.863 ... #>  $ Y    : num  0.874 0.926 1.051 0.918 1.157 ... #>  $ Class: Factor w/ 4 levels \"1\",\"2\",\"3\",\"4\": 1 1 1 1 1 1 1 1 1 1 ... str(synth) #> 'data.frame':    1000 obs. of  3 variables: #>  $ X    : num  -0.89219 -0.74509 -0.00148 -0.11204 -0.39182 ... #>  $ Y    : num  -0.119 0.742 0.617 0.183 -0.926 ... #>  $ Class: Factor w/ 4 levels \"1\",\"3\",\"2\",\"4\": 4 1 2 2 4 4 1 4 3 1 ...  # Put it all together x$Data <- 'Original' synth$Data <- 'Synthetic' df <- rbind(x, synth)  # Plot results ggplot(df, aes(X, Y, color = Class, shape = Class)) +    geom_point() +    facet_wrap(~ Data)"},{"path":"https://bips-hb.github.io/arf/articles/arf.html","id":"conditioning","dir":"Articles","previous_headings":"","what":"Conditioning","title":"Adversarial Random Forests","text":"ARFs can also used compute likelihoods synthesize data conditioning events specify values ranges input features. instance, say want evaluate likelihood samples iris dataset condition Species = 'setosa'. several ways encode evidence, simplest pass partial observation.  expected, measurements non-setosa samples appear much less likely conditioning event. partial observation method passing evidence requires users specify unique feature values conditioning variable. flexible alternative construct data frame conditioning events, potentially including inequalities. example, may want calculate likelihood samples given Species = 'setosa' Petal.Width > 0.3. row treated extra condition, can define intervals putting multiple constraints single variable. Resulting likelihoods computed condition queries drawn setosa flowers petal width interval \\((0.3, 0.5]\\). final method passing evidence directly compute posterior distribution leaves. useful particularly complex conditioning events currently inbuilt interface, polynomial constraints arbitrary propositions disjunctive normal form. case, just require data frame columns f_idx wt. latter sum unity, distribution normalized warning. methods can used conditional sampling well.  conditioning Class = 4, restrict sampling smile , rather eyes nose. Computing conditional expectations similarly straightforward. average \\(X, Y\\) coordinates smile .","code":"# Compute conditional likelihoods evi <- data.frame(Species = 'setosa') ll_conditional <- lik(params_iris, query = iris_without_species, evidence = evi)  # Compare NLL across species (shifting to positive range for visualization) tmp <- iris tmp$NLL <- -ll_conditional + max(ll_conditional) + 1 ggplot(tmp, aes(Species, NLL, fill = Species)) +    geom_boxplot() +    scale_y_log10() +    ylab('Negative Log-Likelihood') +    theme(legend.position = 'none') #> Warning: Removed 3 rows containing non-finite outside the scale range #> (`stat_boxplot()`). # Data frame of conditioning events evi <- data.frame(variable = c('Species', 'Petal.Width'),                   relation = c('==', '>'),                    value = c('setosa', 0.3)) evi #>      variable relation  value #> 1     Species       == setosa #> 2 Petal.Width        >    0.3 evi <- data.frame(variable = c('Species', 'Petal.Width', 'Petal.Width'),                   relation = c('==', '>', '<='),                    value = c('setosa', 0.3, 0.5)) evi #>      variable relation  value #> 1     Species       == setosa #> 2 Petal.Width        >    0.3 #> 3 Petal.Width       <=    0.5 # Drawing random weights evi <- data.frame(f_idx = params_iris$forest$f_idx,                   wt = rexp(nrow(params_iris$forest))) evi$wt <- evi$wt / sum(evi$wt) head(evi) #>   f_idx           wt #> 1     1 1.794477e-05 #> 2     2 2.983682e-04 #> 3     3 2.302738e-06 #> 4     4 1.078006e-03 #> 5     5 4.394211e-04 #> 6     6 4.283289e-04 # Simulate class-conditional data for smiley example evi <- data.frame(Class = 4) synth2 <- forge(params_smiley, n_synth = 250, evidence = evi)  # Put it all together synth2$Data <- 'Synthetic' df <- rbind(x, synth2)  # Plot results ggplot(df, aes(X, Y, color = Class, shape = Class)) +    geom_point() +    facet_wrap(~ Data) expct(params_smiley, evidence = evi) #>             X          Y #> 1 -0.01207206 -0.6734144"},{"path":"https://bips-hb.github.io/arf/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Marvin N. Wright. Author, maintainer. David S. Watson. Author. Kristin Blesch. Author. Jan Kapar. Author.","code":""},{"path":"https://bips-hb.github.io/arf/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Wright M, Watson D, Blesch K, Kapar J (2024). arf: Adversarial Random Forests. R package version 0.2.0,  https://bips-hb.github.io/arf/, https://github.com/bips-hb/arf.","code":"@Manual{,   title = {arf: Adversarial Random Forests},   author = {Marvin N. Wright and David S. Watson and Kristin Blesch and Jan Kapar},   year = {2024},   note = {R package version 0.2.0,  https://bips-hb.github.io/arf/},   url = {https://github.com/bips-hb/arf}, }"},{"path":[]},{"path":"https://bips-hb.github.io/arf/index.html","id":"introduction","dir":"","previous_headings":"","what":"Introduction","title":"Adversarial Random Forests","text":"Adversarial random forests (ARFs) recursively partition data fully factorized leaves, features jointly independent. procedure iterative, alternating rounds generation discrimination. Data become increasingly realistic round, original synthetic samples can longer reliably distinguished. useful several unsupervised learning tasks, density estimation data synthesis. Methods implemented package. ARFs naturally handle unstructured data mixed continuous categorical covariates. inherit many benefits RFs, including speed, flexibility, solid performance default parameters.","code":""},{"path":"https://bips-hb.github.io/arf/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Adversarial Random Forests","text":"arf package available CRAN: install development version GitHub using devtools, run:","code":"install.packages(\"arf\") devtools::install_github(\"bips-hb/arf\")"},{"path":"https://bips-hb.github.io/arf/index.html","id":"examples","dir":"","previous_headings":"","what":"Examples","title":"Adversarial Random Forests","text":"Using Fisher’s iris dataset, train ARF estimate distribution parameters:","code":"# Train the ARF arf <- adversarial_rf(iris)  # Estimate distribution parameters psi <- forde(arf, iris)"},{"path":"https://bips-hb.github.io/arf/index.html","id":"density-estimation","dir":"","previous_headings":"Examples","what":"Density estimation","title":"Adversarial Random Forests","text":"estimate log-likelihoods:","code":"mean(lik(arf, psi, iris))"},{"path":"https://bips-hb.github.io/arf/index.html","id":"generative-modeling","dir":"","previous_headings":"Examples","what":"Generative modeling","title":"Adversarial Random Forests","text":"generate 100 synthetic samples:","code":"forge(psi, 100)"},{"path":"https://bips-hb.github.io/arf/index.html","id":"conditional-expectations","dir":"","previous_headings":"Examples","what":"Conditional expectations","title":"Adversarial Random Forests","text":"estimate mean variable(s), optionally conditioned event(s): detailed examples, see package vignette.","code":"evi <- data.frame(Species = \"setosa\") expct(psi, query = \"Sepal.Length\", evidence = evi)"},{"path":"https://bips-hb.github.io/arf/index.html","id":"python-library","dir":"","previous_headings":"","what":"Python library","title":"Adversarial Random Forests","text":"Python implementation ARF, arfpy, available PyPI. development version, see .","code":""},{"path":"https://bips-hb.github.io/arf/index.html","id":"references","dir":"","previous_headings":"","what":"References","title":"Adversarial Random Forests","text":"Watson, D. S., Blesch, K., Kapar, J. & Wright, M. N. (2023). Adversarial random forests density estimation generative modeling. Proceedings 26th International Conference Artificial Intelligence Statistics. Link .","code":""},{"path":"https://bips-hb.github.io/arf/reference/adversarial_rf.html","id":null,"dir":"Reference","previous_headings":"","what":"Adversarial Random Forests — adversarial_rf","title":"Adversarial Random Forests — adversarial_rf","text":"Implements adversarial random forest learn independence-inducing splits.","code":""},{"path":"https://bips-hb.github.io/arf/reference/adversarial_rf.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Adversarial Random Forests — adversarial_rf","text":"","code":"adversarial_rf(   x,   num_trees = 10L,   min_node_size = 2L,   delta = 0,   max_iters = 10L,   early_stop = TRUE,   prune = TRUE,   verbose = TRUE,   parallel = TRUE,   ... )"},{"path":"https://bips-hb.github.io/arf/reference/adversarial_rf.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Adversarial Random Forests — adversarial_rf","text":"x Input data. Integer variables recoded ordered factors warning. See Details. num_trees Number trees grow forest. default works well generative modeling tasks, increased likelihood estimation. See Details. min_node_size Minimal number real data samples leaf nodes. delta Tolerance parameter. Algorithm converges OOB accuracy < 0.5 + delta. max_iters Maximum iterations adversarial loop. early_stop Terminate loop performance fails improve one round next? prune Impose min_node_size pruning? verbose Print discriminator accuracy round? parallel Compute parallel? Must register backend beforehand, e.g. via doParallel. ... Extra parameters passed ranger.","code":""},{"path":"https://bips-hb.github.io/arf/reference/adversarial_rf.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Adversarial Random Forests — adversarial_rf","text":"random forest object class ranger.","code":""},{"path":"https://bips-hb.github.io/arf/reference/adversarial_rf.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Adversarial Random Forests — adversarial_rf","text":"adversarial random forest (ARF) algorithm partitions data fully factorized leaves features jointly independent. ARFs trained iteratively, alternating rounds generation discrimination. first instance, synthetic data generated via independent bootstraps feature, RF classifier trained distinguish real fake samples. subsequent rounds, synthetic data generated separately leaf, using splits previous forest. creates increasingly realistic data satisfies local independence construction. algorithm converges RF reliably distinguish two classes, .e. OOB accuracy falls 0.5 + delta. ARFs useful several unsupervised learning tasks, density estimation (see forde) data synthesis (see forge). former, recommend increasing number trees improved performance (typically order 100-1000 depending sample size). Integer variables recoded warning. Default behavior convert six unique values numeric, five unique values treated ordered factors. override behavior, explicitly recode integer variables target type prior training. Note: convergence guaranteed finite samples. max_iters argument sets upper bound number training rounds. Similar results may attained increasing delta. Even single round can often give good performance, data strong complex dependencies may require iterations. default early_stop = TRUE, adversarial loop terminates performance improve one round next, case training may pointless.","code":""},{"path":"https://bips-hb.github.io/arf/reference/adversarial_rf.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Adversarial Random Forests — adversarial_rf","text":"Watson, D., Blesch, K., Kapar, J., & Wright, M. (2023). Adversarial random forests density estimation generative modeling. Proceedings 26th International Conference Artificial Intelligence Statistics, pp. 5357-5375.","code":""},{"path":[]},{"path":"https://bips-hb.github.io/arf/reference/adversarial_rf.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Adversarial Random Forests — adversarial_rf","text":"","code":"arf <- adversarial_rf(iris) #> Iteration: 0, Accuracy: 84.85% #> Iteration: 1, Accuracy: 40.54% #> Warning: executing %dopar% sequentially: no parallel backend registered"},{"path":"https://bips-hb.github.io/arf/reference/col_rename.html","id":null,"dir":"Reference","previous_headings":"","what":"Adaptive column renaming — col_rename","title":"Adaptive column renaming — col_rename","text":"function renames columns case input data.frame includes colnames required internal functions (e.g., \"y\").","code":""},{"path":"https://bips-hb.github.io/arf/reference/col_rename.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Adaptive column renaming — col_rename","text":"","code":"col_rename(df, old_name)"},{"path":"https://bips-hb.github.io/arf/reference/col_rename.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Adaptive column renaming — col_rename","text":"df Input data.frame. old_name Name column renamed.","code":""},{"path":"https://bips-hb.github.io/arf/reference/expct.html","id":null,"dir":"Reference","previous_headings":"","what":"Expected Value — expct","title":"Expected Value — expct","text":"Compute expectation query variable(s), optionally conditioned event(s).","code":""},{"path":"https://bips-hb.github.io/arf/reference/expct.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Expected Value — expct","text":"","code":"expct(params, query = NULL, evidence = NULL)"},{"path":"https://bips-hb.github.io/arf/reference/expct.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Expected Value — expct","text":"params Circuit parameters learned via forde. query Optional character vector variable names. Estimates computed . NULL, variables evidence estimated. evidence Optional set conditioning events. can take one three forms: (1) partial sample, .e. single row data columns; (2) data frame conditioning events, allows inequalities; (3) posterior distribution leaves. See Details.","code":""},{"path":"https://bips-hb.github.io/arf/reference/expct.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Expected Value — expct","text":"one row data frame values query variables.","code":""},{"path":"https://bips-hb.github.io/arf/reference/expct.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Expected Value — expct","text":"function computes expected values subset features, optionally conditioned event(s).","code":""},{"path":"https://bips-hb.github.io/arf/reference/expct.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Expected Value — expct","text":"Watson, D., Blesch, K., Kapar, J., & Wright, M. (2023). Adversarial random forests density estimation generative modeling. Proceedings 26th International Conference Artificial Intelligence Statistics, pp. 5357-5375.","code":""},{"path":[]},{"path":"https://bips-hb.github.io/arf/reference/expct.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Expected Value — expct","text":"","code":"# Train ARF and corresponding circuit arf <- adversarial_rf(iris) #> Iteration: 0, Accuracy: 82.43% #> Iteration: 1, Accuracy: 38.38% psi <- forde(arf, iris)  # What is the expected value Sepal.Length? expct(psi, query = \"Sepal.Length\") #>   Sepal.Length #> 1          5.8  # What if we condition on Species = \"setosa\"? evi <- data.frame(Species = \"setosa\") expct(psi, query = \"Sepal.Length\", evidence = evi) #>   Sepal.Length #> 1            5  # Compute expectations for all features other than Species expct(psi, evidence = evi) #>   Sepal.Length Sepal.Width Petal.Length Petal.Width #> 1            5         3.4          1.5         0.3"},{"path":"https://bips-hb.github.io/arf/reference/forde.html","id":null,"dir":"Reference","previous_headings":"","what":"Forests for Density Estimation — forde","title":"Forests for Density Estimation — forde","text":"Uses pre-trained ARF model estimate leaf distribution parameters.","code":""},{"path":"https://bips-hb.github.io/arf/reference/forde.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Forests for Density Estimation — forde","text":"","code":"forde(   arf,   x,   oob = FALSE,   family = \"truncnorm\",   finite_bounds = FALSE,   alpha = 0,   epsilon = 0,   parallel = TRUE )"},{"path":"https://bips-hb.github.io/arf/reference/forde.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Forests for Density Estimation — forde","text":"arf Pre-trained adversarial_rf. Alternatively, object class ranger. x Training data estimating parameters. oob use --bag samples parameter estimation? TRUE, x must dataset used train arf. family Distribution use density estimation continuous features. Current options include truncated normal (default family = \"truncnorm\") uniform (family = \"unif\"). See Details. finite_bounds Impose finite bounds continuous variables? alpha Optional pseudocount Laplace smoothing categorical features. avoids zero-mass points test data fall outside support training data. Effectively parametrizes flat Dirichlet prior multinomial likelihoods. epsilon Optional slack parameter empirical bounds family = \"unif\" finite_bounds = TRUE. avoids zero-density points test data fall outside support training data. gap lower upper bounds expanded factor 1 + epsilon. parallel Compute parallel? Must register backend beforehand, e.g. via doParallel.","code":""},{"path":"https://bips-hb.github.io/arf/reference/forde.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Forests for Density Estimation — forde","text":"list 5 elements: (1) parameters continuous data; (2) parameters discrete data; (3) leaf indices coverage; (4) metadata variables; (5) data input class. list used estimating likelihoods lik generating data forge.","code":""},{"path":"https://bips-hb.github.io/arf/reference/forde.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Forests for Density Estimation — forde","text":"forde extracts leaf parameters pretrained forest learns distribution parameters data within leaf. former includes coverage (proportion data falling leaf) split criteria. latter includes proportions categorical features mean/variance continuous features. result probabilistic circuit, stored data.table, can used various downstream inference tasks. Currently, forde provides support limited number distributional families: truncated normal uniform continuous data, multinomial discrete data. Future releases accommodate larger set options. Though forde designed take adversarial random forest input, function's first argument can principle object class ranger. allows users test performance alternative pipelines (e.g., supervised forest input). also requirement x data used fit arf, unless oob = TRUE. fact, using another dataset may protect overfitting. connects Wager & Athey's (2018) notion \"honest trees\".","code":""},{"path":"https://bips-hb.github.io/arf/reference/forde.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Forests for Density Estimation — forde","text":"Watson, D., Blesch, K., Kapar, J., & Wright, M. (2023). Adversarial random forests density estimation generative modeling. Proceedings 26th International Conference Artificial Intelligence Statistics, pp. 5357-5375. Wager, S. & Athey, S. (2018). Estimation inference heterogeneous treatment effects using random forests. J. . Stat. Assoc., 113(523): 1228-1242.","code":""},{"path":[]},{"path":"https://bips-hb.github.io/arf/reference/forde.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Forests for Density Estimation — forde","text":"","code":"arf <- adversarial_rf(iris) #> Iteration: 0, Accuracy: 86.05% #> Iteration: 1, Accuracy: 45.67% psi <- forde(arf, iris) head(psi) #> $cnt #> Key: <f_idx, variable> #>      f_idx     variable   min   max    mu      sigma #>      <int>       <char> <num> <num> <num>      <num> #>   1:     1 Petal.Length  -Inf  1.55  1.35 0.07071068 #>   2:     1  Petal.Width  -Inf   Inf  0.25 0.07071068 #>   3:     1 Sepal.Length  -Inf   Inf  4.45 0.07071068 #>   4:     1  Sepal.Width  -Inf  2.95  2.60 0.42426407 #>   5:     2 Petal.Length  4.05  4.30  4.15 0.07071068 #>  ---                                                 #> 720:   180  Sepal.Width  2.90  3.45  3.40 0.11456157 #> 721:   181 Petal.Length  -Inf   Inf  4.96 0.13416408 #> 722:   181  Petal.Width  1.65   Inf  1.96 0.26076810 #> 723:   181 Sepal.Length  -Inf  6.15  5.88 0.19235384 #> 724:   181  Sepal.Width  2.75   Inf  2.92 0.10954451 #>  #> $cat #> Key: <f_idx, variable> #>      f_idx variable        val  prob #>      <int>   <char>     <char> <num> #>   1:     1  Species     setosa   1.0 #>   2:     2  Species versicolor   1.0 #>   3:     3  Species versicolor   1.0 #>   4:     4  Species versicolor   0.5 #>   5:     4  Species  virginica   0.5 #>  ---                                 #> 211:   177  Species versicolor   1.0 #> 212:   178  Species     setosa   1.0 #> 213:   179  Species  virginica   1.0 #> 214:   180  Species     setosa   1.0 #> 215:   181  Species  virginica   1.0 #>  #> $forest #> Key: <tree, leaf> #>      f_idx  tree  leaf        cvg #>      <int> <int> <int>      <num> #>   1:     1     1    14 0.01333333 #>   2:     2     1    20 0.01333333 #>   3:     3     1    26 0.10000000 #>   4:     4     1    28 0.01333333 #>   5:     5     1    29 0.02000000 #>  ---                              #> 177:   177    10    48 0.03333333 #> 178:   178    10    50 0.15333333 #> 179:   179    10    52 0.02666667 #> 180:   180    10    55 0.02000000 #> 181:   181    10    56 0.03333333 #>  #> $meta #>        variable   class    family decimals #>          <char>  <char>    <char>    <int> #> 1: Sepal.Length numeric truncnorm        1 #> 2:  Sepal.Width numeric truncnorm        1 #> 3: Petal.Length numeric truncnorm        1 #> 4:  Petal.Width numeric truncnorm        1 #> 5:      Species  factor  multinom       NA #>  #> $levels #>    variable        val #>      <fctr>     <char> #> 1:  Species  virginica #> 2:  Species     setosa #> 3:  Species versicolor #>  #> $input_class #> [1] \"data.frame\" #>"},{"path":"https://bips-hb.github.io/arf/reference/forge.html","id":null,"dir":"Reference","previous_headings":"","what":"Forests for Generative Modeling — forge","title":"Forests for Generative Modeling — forge","text":"Uses pre-trained FORDE model simulate synthetic data.","code":""},{"path":"https://bips-hb.github.io/arf/reference/forge.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Forests for Generative Modeling — forge","text":"","code":"forge(params, n_synth, evidence = NULL)"},{"path":"https://bips-hb.github.io/arf/reference/forge.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Forests for Generative Modeling — forge","text":"params Circuit parameters learned via forde. n_synth Number synthetic samples generate. evidence Optional set conditioning events. can take one three forms: (1) partial sample, .e. single row data columns; (2) data frame conditioning events, allows inequalities; (3) posterior distribution leaves. See Details.","code":""},{"path":"https://bips-hb.github.io/arf/reference/forge.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Forests for Generative Modeling — forge","text":"dataset n_synth synthetic samples.","code":""},{"path":"https://bips-hb.github.io/arf/reference/forge.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Forests for Generative Modeling — forge","text":"forge simulates synthetic dataset n_synth samples. First, leaves sampled proportion either coverage (evidence = NULL) posterior probability. , feature sampled independently within leaf according probability mass density function learned forde. create realistic data long adversarial RF used previous step satisfies local independence criterion. See Watson et al. (2023). three methods (optionally) encoding conditioning events via evidence argument. first provide partial sample, columns training data present. second provide data frame three columns: variable, relation, value. supports inequalities via relation. Alternatively, users may directly input pre-calculated posterior distribution leaves, columns f_idx wt. may preferable complex constraints. See Examples.","code":""},{"path":"https://bips-hb.github.io/arf/reference/forge.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Forests for Generative Modeling — forge","text":"Watson, D., Blesch, K., Kapar, J., & Wright, M. (2023). Adversarial random forests density estimation generative modeling. Proceedings 26th International Conference Artificial Intelligence Statistics, pp. 5357-5375.","code":""},{"path":[]},{"path":"https://bips-hb.github.io/arf/reference/forge.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Forests for Generative Modeling — forge","text":"","code":"arf <- adversarial_rf(iris) #> Iteration: 0, Accuracy: 83.16% #> Iteration: 1, Accuracy: 42.62% psi <- forde(arf, iris) x_synth <- forge(psi, n_synth = 100)  # Condition on Species = \"setosa\" evi <- data.frame(Species = \"setosa\") x_synth <- forge(psi, n_synth = 100, evidence = evi)  # Condition in Species = \"setosa\" and Sepal.Length > 6 evi <- data.frame(variable = c(\"Species\", \"Sepal.Length\"),                   relation = c(\"==\", \">\"),                    value = c(\"setosa\", 6)) x_synth <- forge(psi, n_synth = 100, evidence = evi)  # Or just input some distribution on leaves # (Weights that do not sum to unity are automatically scaled) n_leaves <- nrow(psi$forest) evi <- data.frame(f_idx = psi$forest$f_idx, wt = rexp(n_leaves)) x_synth <- forge(psi, n_synth = 100, evidence = evi) #> Warning: Posterior weights have been normalized to sum to unity."},{"path":"https://bips-hb.github.io/arf/reference/leaf_posterior.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute leaf posterior — leaf_posterior","title":"Compute leaf posterior — leaf_posterior","text":"function returns posterior distribution leaves, conditional evidence.","code":""},{"path":"https://bips-hb.github.io/arf/reference/leaf_posterior.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute leaf posterior — leaf_posterior","text":"","code":"leaf_posterior(params, evidence)"},{"path":"https://bips-hb.github.io/arf/reference/leaf_posterior.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute leaf posterior — leaf_posterior","text":"params Circuit parameters learned via forde. evidence Data frame conditioning event(s).","code":""},{"path":"https://bips-hb.github.io/arf/reference/lik.html","id":null,"dir":"Reference","previous_headings":"","what":"Likelihood Estimation — lik","title":"Likelihood Estimation — lik","text":"Compute likelihood input data, optionally conditioned event(s).","code":""},{"path":"https://bips-hb.github.io/arf/reference/lik.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Likelihood Estimation — lik","text":"","code":"lik(   params,   query,   evidence = NULL,   arf = NULL,   oob = FALSE,   log = TRUE,   batch = NULL,   parallel = TRUE )"},{"path":"https://bips-hb.github.io/arf/reference/lik.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Likelihood Estimation — lik","text":"params Circuit parameters learned via forde. query Data frame samples, optionally comprising just subset training features. Likelihoods computed sample. Missing features marginalized . See Details. evidence Optional set conditioning events. can take one three forms: (1) partial sample, .e. single row data columns; (2) data frame conditioning events, allows inequalities; (3) posterior distribution leaves. See Details. arf Pre-trained adversarial_rf object class ranger. required speeds computation considerably total evidence queries. (Ignored partial evidence queries.) oob use --bag leaves likelihood estimation? TRUE, x must dataset used train arf. applicable total evidence queries. log Return likelihoods log scale? Recommended prevent underflow. batch Batch size. default compute densities queries one round, always fastest option memory allows. However, large samples many trees, can memory efficient split data batches. impact results. parallel Compute parallel? Must register backend beforehand, e.g. via doParallel.","code":""},{"path":"https://bips-hb.github.io/arf/reference/lik.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Likelihood Estimation — lik","text":"vector likelihoods, optionally log scale.","code":""},{"path":"https://bips-hb.github.io/arf/reference/lik.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Likelihood Estimation — lik","text":"function computes likelihood input data, optionally conditioned event(s). Queries may partial, .e. covering features, case excluded variables marginalized . three methods (optionally) encoding conditioning events via evidence argument. first provide partial sample, columns training data present. second provide data frame three columns: variable, relation, value. supports inequalities via relation. Alternatively, users may directly input pre-calculated posterior distribution leaves, columns f_idx wt. may preferable complex constraints. See Examples.","code":""},{"path":"https://bips-hb.github.io/arf/reference/lik.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Likelihood Estimation — lik","text":"Watson, D., Blesch, K., Kapar, J., & Wright, M. (2023). Adversarial random forests density estimation generative modeling. Proceedings 26th International Conference Artificial Intelligence Statistics, pp. 5357-5375.","code":""},{"path":[]},{"path":"https://bips-hb.github.io/arf/reference/lik.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Likelihood Estimation — lik","text":"","code":"# Estimate average log-likelihood arf <- adversarial_rf(iris) #> Iteration: 0, Accuracy: 85.14% #> Iteration: 1, Accuracy: 41.28% psi <- forde(arf, iris) ll <- lik(psi, iris, arf = arf, log = TRUE) mean(ll) #> [1] -0.8934279  # Identical but slower ll <- lik(psi, iris, log = TRUE) #> Warning: For total evidence queries, it is faster to include the pre-trained arf. mean(ll) #> [1] -0.8934279  # Partial evidence query lik(psi, query = iris[1, 1:3]) #> [1] 0.1627729  # Condition on Species = \"setosa\" evi <- data.frame(Species = \"setosa\") lik(psi, query = iris[1, 1:3], evidence = evi) #> [1] 1.261312  # Condition on Species = \"setosa\" and Petal.Width > 0.3 evi <- data.frame(variable = c(\"Species\", \"Petal.Width\"),                   relation = c(\"==\", \">\"),                    value = c(\"setosa\", 0.3)) lik(psi, query = iris[1, 1:3], evidence = evi) #> [1] 0.918821"},{"path":"https://bips-hb.github.io/arf/reference/post_x.html","id":null,"dir":"Reference","previous_headings":"","what":"Post-process data — post_x","title":"Post-process data — post_x","text":"function prepares output data forge.","code":""},{"path":"https://bips-hb.github.io/arf/reference/post_x.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Post-process data — post_x","text":"","code":"post_x(x, params)"},{"path":"https://bips-hb.github.io/arf/reference/post_x.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Post-process data — post_x","text":"x Input data.frame. params Circuit parameters learned via forde.","code":""},{"path":"https://bips-hb.github.io/arf/reference/prep_evi.html","id":null,"dir":"Reference","previous_headings":"","what":"Preprocess evidence — prep_evi","title":"Preprocess evidence — prep_evi","text":"function prepares evidence computing leaf posteriors.","code":""},{"path":"https://bips-hb.github.io/arf/reference/prep_evi.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Preprocess evidence — prep_evi","text":"","code":"prep_evi(params, evidence)"},{"path":"https://bips-hb.github.io/arf/reference/prep_evi.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Preprocess evidence — prep_evi","text":"params Circuit parameters learned via forde. evidence Optional set conditioning events.","code":""},{"path":"https://bips-hb.github.io/arf/reference/prep_x.html","id":null,"dir":"Reference","previous_headings":"","what":"Preprocess input data — prep_x","title":"Preprocess input data — prep_x","text":"function prepares input data ARFs.","code":""},{"path":"https://bips-hb.github.io/arf/reference/prep_x.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Preprocess input data — prep_x","text":"","code":"prep_x(x)"},{"path":"https://bips-hb.github.io/arf/reference/prep_x.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Preprocess input data — prep_x","text":"x Input data.frame.","code":""},{"path":"https://bips-hb.github.io/arf/news/index.html","id":"arf-020","dir":"Changelog","previous_headings":"","what":"arf 0.2.0","title":"arf 0.2.0","text":"CRAN release: 2024-01-24 Vectorized adversarial resampling Speed boost compiling probabilistic circuit Conditional densities sampling Bayesian solution invariant continuous data within leaf nodes New function computing (conditional) expectations Options missing data","code":""},{"path":"https://bips-hb.github.io/arf/news/index.html","id":"arf-013","dir":"Changelog","previous_headings":"","what":"arf 0.1.3","title":"arf 0.1.3","text":"CRAN release: 2023-02-06 Speed boost adversarial resampling step Early stopping option adversarial training alpha parameter regularizing multinomial distributions forde Unified treatment colnames internal semantics (y, obs, tree, leaf)","code":""}]
